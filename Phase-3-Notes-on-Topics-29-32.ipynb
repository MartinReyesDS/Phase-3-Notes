{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Topics 29 - 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 29: Decision Trees\n",
    "### Intro to Decision Trees\n",
    "- “recursive binary splitting\" The process of training a decision tree and predicting the target features of a dataset is as follows:\n",
    "    1. Present a dataset of training examples containing features/predictors and a target (similar to classifiers we have seen earlier).\n",
    "    2. Train the tree model by making splits for the target using the values of predictors. Which features to use as predictors gets selected following the idea of feature selection and uses measures like \"information gain\" and \"Gini Index\". We shall cover these shortly.\n",
    "    3. The tree is grown until some stopping criteria is achieved. This could be a set depth of the tree or any other similar measure.\n",
    "    4. Show a new set of features to the tree, with an unknown class and let the example propagate through a trained tree. The resulting leaf node represents the class prediction for this example datum.\n",
    "- CART (Classification and Regression Trees) uses the Gini Index as a metric\n",
    "- ID3 (Iterative Dichotomiser 3) uses the entropy function and information gain as metrics\n",
    "- $Entropy(p) = -\\sum (P_i . log_2(P_i))$\n",
    "- High entropy means less predictive power\n",
    "- As input, the function should take in `D` as a class distribution array for target class, and `a` the class distribution of the attribute to be tested, then calculate gain as $gain(D,A) = Entropy(D) - \\sum(\\frac{|D_i|}{|D|}.Entropy(D_i))$, where $D_{i}$ represents distribution of each class in `a`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Classification Trees: Perfect Split with Information Gain Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T23:43:43.661937Z",
     "start_time": "2021-02-05T23:43:43.655294Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "def entropy(pi):\n",
    "    \"\"\"\n",
    "    return the Entropy of a probability distribution:\n",
    "    entropy(p) = - SUM (Pi * log(Pi) )\n",
    "    pi is a list of how many occurances there are in each class\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for p in pi:\n",
    "        p = p / sum(pi)\n",
    "        if p != 0:\n",
    "            total +=  p * log(p, 2)\n",
    "        else:\n",
    "            total += 0\n",
    "    total *= -1\n",
    "    return total\n",
    "def IG(D, a):\n",
    "    \"\"\"\n",
    "    return the information gain:\n",
    "    gain(D, A) = entropy(D)− SUM( |Di| / |D| * entropy(Di) )\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for Di in a:\n",
    "        total += abs(sum(Di) / sum(D)) * entropy(Di)\n",
    "    gain = entropy(D) - total\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Trees using scikit-learn + Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import tree\n",
    "# One-hot encode the training data and show the resulting DataFrame with proper column names\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train_ohe = ohe.transform(X_train).toarray()\n",
    "# Creating this DataFrame is not necessary its only to show the result of the ohe\n",
    "ohe_df = pd.DataFrame(X_train_ohe, columns=ohe.get_feature_names(X_train.columns))\n",
    "ohe_df.head()\n",
    "# Create the classifier, fit it on the training data and make predictions on the test set\n",
    "clf = DecisionTreeClassifier(criterion='entropy') # or 'gini'\n",
    "clf.fit(X_train_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1,ncols = 1, figsize = (3,3), dpi=300)\n",
    "tree.plot_tree(clf,\n",
    "               feature_names = ohe_df.columns, \n",
    "               class_names=np.unique(y).astype('str'),\n",
    "               filled = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ohe = ohe.transform(X_test)\n",
    "y_preds = clf.predict(X_test_ohe)\n",
    "\n",
    "# Calculate accuracy \n",
    "acc = accuracy_score(y_test,y_pred) * 100\n",
    "print('Accuracy is :{0}'.format(acc))\n",
    "\n",
    "# Check the AUC for predictions\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print('\\nAUC is :{0}'.format(round(roc_auc, 2)))\n",
    "\n",
    "# Create and print a confusion matrix \n",
    "print('\\nConfusion Matrix')\n",
    "print('----------------')\n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "# or\n",
    "# Alternative confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(classifier, X, y, values_format='.3g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning and Pruning in Decision Trees + Lab\n",
    "We can prune our trees using:\n",
    "- Maximum depth: Reduce the depth of the tree to build a generalized tree. Set the depth of the tree to 3, 5, 10 depending after verification on test data\n",
    "- Minimum samples leaf with split: minimum number of samples required to split an internal node.\n",
    "- max_depth and min_samples_split are also both related to the computational cost\n",
    "- Minimum leaf sample size: minimum number of samples that we want a leaf node to contain. When this minimum size is achieved at a nodE. Size in terminal nodes can be fixed to 30, 100, 300 or 5% of total\n",
    "- Maximum leaf nodes: Reduce the number of leaf nodes\n",
    "- Maximum features: Maximum number of features to consider when splitting a node\n",
    "- The main difference between the two is that min_samples_leaf guarantees a minimum number of samples in a leaf, while min_samples_split can create arbitrary small leaves, though min_samples_split is more common in practice\n",
    "\n",
    "- For instance, if min_samples_split = 5, and there are 7 samples at an internal node, then the split is allowed. But let's say the split results in two leaves, one with 1 sample, and another with 6 samples. If min_samples_leaf = 2, then the split won't be allowed (even if the internal node has 7 samples) because one of the leaves resulted will have less than the minimum number of samples required to be at a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(criterion='entropy', max_features=6, max_depth=3,\n",
    "                            min_samples_split=0.7, min_samples_leaf=0.25, random_state=SEED)\n",
    "dt.fit(X_train, y_train)\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with CART Trees + Lab\n",
    "- Recursive partitioning, instead of global model.\n",
    "- Cost Function: $J(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}$\n",
    "    - $D$: remaining training examples   \n",
    "    - $n_{total}$ : number of remaining training examples\n",
    "    - $\\theta = (f, t_f)$: feature and feature threshold\n",
    "    - $n_{left}/n_{right}$: number of samples in the left/right subset\n",
    "    - $MSE_{left}/MSE_{right}$: MSE of the left/right subset\n",
    "- $ \\hat{y}_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} y_i $\n",
    "$ MSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2 $\n",
    "    - $D_m$: training examples in node $m$\n",
    "    - $n_{m}$ : total number of training examples in node $m$\n",
    "    - $y_i$: target value of $i-$th example\n",
    "- Without regularization, decision trees are likely to overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state=42, max_depth=3)\n",
    "regressor.fit(X_train, y_train)\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import r2_score\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('MAE:', mae(y_test, y_pred))\n",
    "print('MSE:', mse(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(mse(y_test, y_pred)))\n",
    "print('R-sq score:', r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Trees and Model Optimization Lab\n",
    "\n",
    "- Go to lab to see how to tune parameters manually and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for each feature vs. target, can add title, lables, and tight_layout\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i, col in enumerate(features.columns):\n",
    "    plt.subplot(len(features.columns)/3, len(features.columns), i+1)\n",
    "    plt.plot(data[col], target, 'o')\n",
    "def performance(y_true, y_predict):\n",
    "    r2 = r2_score(y_true, y_predict)\n",
    "    rmse = mean_squared_error(y_true, y_predict, squared=False)\n",
    "    return [r2, rmse]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 30: Ensemble Methods\n",
    "\n",
    "- Bagging, short for Bootstrap Aggregation, is a combination of two ideas -- bootstrap resampling and aggregation\n",
    "- common approach is to treat each classifier in the ensemble's prediction as a \"vote\" and let our overall prediction be the majority vote\n",
    "- also common to see ensembles that take the arithmetic mean of all predictions, or compute a weighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "- classification and regression, ensemble of decision trees \n",
    "- Bagging: sample two-thirds of our training data with replacement\n",
    "- the algorithm then uses the remaining one-third of data that wasn't sampled to calculate the Out-Of-Bag Error\n",
    "- \n",
    "- Subspace sampling method: randomly select a subset of features (exact number is tunable parameter) to use as predictors for each node when training a decision tree\n",
    "- Benefits: Strong performance and interpretability \n",
    "- Drawbacks: Computational complexity and memory storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Ensembles and Random Forests Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "tree_clf = DecisionTreeClassifier(criterion='gini', max_depth=5) # Regular Tree\n",
    "tree_clf.fit(data_train, target_train)\n",
    "tree_clf.feature_importances_ # Can plot (In lab)\n",
    "pred = tree_clf.predict(data_test)\n",
    "print(confusion_matrix(target_test, pred))\n",
    "print(classification_report(target_test, pred))\n",
    "print(accuracy_score(target_test, pred) * 100)\n",
    "bagged_tree =  BaggingClassifier(DecisionTreeClassifier(criterion='gini', max_depth=5), \n",
    "                                 n_estimators=20) # BaggingClassifier\n",
    "bagged_tree.fit(data_train, target_train)\n",
    "ran_forest = RandomForestClassifier(n_estimators=100, max_depth= 5) # RandomForestClassifier\n",
    "ran_forest.fit(data_train, target_train)\n",
    "forest_2 = RandomForestClassifier(n_estimators = 5, max_features= 10, max_depth= 2)\n",
    "forest_2.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV + Lab\n",
    "GridSearchCV: combines K-Fold Cross-Validation with a grid search of parameters\n",
    "- num models = num_cv + product of parameters\n",
    "- very time consuming and computationally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [1, 2, 5, 10],\n",
    "    'min_samples_split': [1, 5, 10, 20]\n",
    "}\n",
    "gs_tree = GridSearchCV(clf, param_grid, cv=3)\n",
    "gs_tree.fit(train_data, train_labels)\n",
    "gs_tree.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\\\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_cv_score = cross_val_score(dt_clf, X_train, y_train, cv=3)\n",
    "mean_dt_cv_score = np.mean(dt_cv_score)\n",
    "print(f\"Mean Cross Validation Score: {mean_dt_cv_score :.2%}\")\n",
    "dt_param_grid = {'criterion': ['gini', 'entropy'], 'max_depth': [None, 2, 3, 4, 5, 6],\n",
    "                    'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 3, 4, 5, 6]}\n",
    "dt_grid_search = GridSearchCV(dt_clf, dt_param_grid, cv=3, return_train_score=True)\n",
    "dt_grid_search.fit(X_train, y_train)\n",
    "dt_gs_training_score = np.mean(dt_grid_search.cv_results_['mean_train_score'])\n",
    "dt_gs_testing_score = dt_grid_search.score(X_test, y_test)\n",
    "print(f\"Mean Training Score: {dt_gs_training_score :.2%}\")\n",
    "print(f\"Mean Test Score: {dt_gs_testing_score :.2%}\")\n",
    "print(\"Best Parameter Combination Found During Grid Search:\")\n",
    "dt_grid_search.best_score_\n",
    "dt_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting and Weak Learners + Lab\n",
    "Boosting works as follows:\n",
    "1. Train a single weak learner\n",
    "2. Figure out which examples the weak learner got wrong\n",
    "3. Build another weak learner that focuses on the areas the first weak learner got wrong\n",
    "4. Continue this process until a predetermined stopping condition is met, such as until a set number of weak learners have been created, or the model's performance has plateaued\n",
    "\n",
    "Adaboost:\n",
    "- Trained on subset of training sample w/ replacement like bagging, except each data point carries a weight. Weight increases when weak longer misclassifies. Each weight acts as a probability that sample will go into bag\n",
    "\n",
    "Gradient Boosted Trees:\n",
    "- Starts with a weak learning and then calculates the Residuals for each data point.\n",
    "- Model combines the Residuals with a differentiable loss function to calculate the overall loss\n",
    "- Use gradients and the loss as predictors to train the next tree against. \n",
    "- Next learner focuses on these harder examples. Loss is reduced because these more commonly misclassified data points are focused on.\n",
    "- Gamma, learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "adaboost_clf = AdaBoostClassifier(random_state=42)\n",
    "gbt_clf = GradientBoostingClassifier(random_state=42)\n",
    "adaboost_train_preds = adaboost_clf.predict(X_train)\n",
    "adaboost_test_preds = adaboost_clf.predict(X_test)\n",
    "gbt_clf_train_preds = gbt_clf.predict(X_train)\n",
    "gbt_clf_test_preds = gbt_clf.predict(X_test)\n",
    "def display_acc_and_f1_score(true, preds, model_name):\n",
    "    acc = accuracy_score(true, preds)\n",
    "    f1 = f1_score(true, preds)\n",
    "    print(\"Model: {}\".format(model_name))\n",
    "    print(\"Accuracy: {}\".format(acc))\n",
    "    print(\"F1-Score: {}\".format(f1))\n",
    "adaboost_confusion_matrix = confusion_matrix(y_test, adaboost_test_preds)\n",
    "gbt_confusion_matrix = confusion_matrix(y_test, gbt_clf_test_preds)\n",
    "adaboost_classification_report = classification_report(y_test, adaboost_test_preds)\n",
    "gbt_classification_report = classification_report(y_test, gbt_clf_test_preds)\n",
    "print('Mean Adaboost Cross-Val Score (k=5):',\n",
    "      cross_val_score(adaboost_clf, df, target, cv=5).mean())\n",
    "print('Mean GBT Cross-Val Score (k=5):',\n",
    "      cross_val_score(gbt_clf, df, target, cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "- parallelizes the construction of trees across all your computer's CPU cores during the training phase. It also allows for more advanced use cases\n",
    "- automatically handles missing values\n",
    "- `conda install py-xgboost`\n",
    "- [XGBoost](https://xgboost.readthedocs.io/en/latest/)\n",
    "- [XG Boost Params](https://xgboost.readthedocs.io/en/latest/parameter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "training_preds = clf.predict(X_train)\n",
    "test_preds = clf.predict(X_test)\n",
    "training_accuracy = accuracy_score(y_train, training_preds)\n",
    "test_accuracy = accuracy_score(y_test, test_preds)\n",
    "# Tuning\n",
    "param_grid = {'learning_rate': [0.1, 0.2], 'max_depth': [6],\n",
    "    'min_child_weight': [1, 2], 'subsample': [0.5, 0.7], 'n_estimators': [100]}\n",
    "grid_clf = GridSearchCV(clf, param_grid, scoring='accuracy', cv=None, n_jobs=1)\n",
    "grid_clf.fit(X_train, y_train)\n",
    "best_parameters = grid_clf.best_params_\n",
    "print('Grid Search found the following optimal parameters: ')\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('%s: %r' % (param_name, best_parameters[param_name]))\n",
    "training_preds = grid_clf.predict(X_train)\n",
    "test_preds = grid_clf.predict(X_test)\n",
    "training_accuracy = accuracy_score(y_train, training_preds)\n",
    "test_accuracy = accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 31: Support Vector Machines\n",
    "- Max Margin classifier: aim to maximize the margin\n",
    "    - We're trying to solve:\n",
    "        - $ w_T x^{(i)} + b \\geq 1$  if $y ^{(i)} = 1$\n",
    "        - $ w_T x^{(i)} + b \\leq -1$  if $y ^{(i)} = -1$\n",
    "        - the objective function to maximize: $\\dfrac{2}{\\lVert w \\rVert}$, or minimize $\\lVert w \\rVert$\n",
    "    - $w_T$ term is the **weight vector**, $b$ term is called the **bias** \n",
    "- Soft Margin classifier:\n",
    "    - $ b + w_Tx^{(i)} \\geq 1-\\xi^{(i)}$ if $y ^{(i)} = 1$\n",
    "    - $ b + w_Tx^{(i)} \\leq -1+\\xi^{(i)}$ if $y ^{(i)} = -1$\n",
    "    - the objective function to minimize: $\\dfrac{1}{2}\\lVert w \\rVert^2+ C(\\sum_i \\xi^{(i)})$\n",
    "    - $\\xi^{(i)}$ is a **slack variable**\n",
    "- Hyperplane defined by weight vector $w_T$ and bias $b$\n",
    "\n",
    "- standardize the data\n",
    "\n",
    "- [kernels](https://scikit-learn.org/stable/modules/svm.html#kernel-functions)\n",
    "- [svm](https://scikit-learn.org/stable/modules/svm.html)\n",
    "- [svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "- [svm.LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
    "- [svm.NuSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T00:37:29.499253Z",
     "start_time": "2021-02-11T00:37:29.497012Z"
    }
   },
   "outputs": [],
   "source": [
    "# from scratch lab shows how to plot boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an SVM using scikit-learn Lab\n",
    "- See lab to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T00:51:00.212920Z",
     "start_time": "2021-02-11T00:50:54.245020Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_1, y_1)\n",
    "clf.coef_\n",
    "clf.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you dig deeper into scikit-learn, you'll notice that there are several ways to create linear SVMs for classification:\n",
    "\n",
    "- `SVC(kernel = \"linear\")` , what you've used above. Documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)  \n",
    "- `svm.LinearSVC()`, which is very similar to the simple SVC method, but:\n",
    "    - Does not allow for the keyword \"kernel\", as it is assumed to be linear (more on non-linear kernels later)\n",
    "    - In the objective function, `LinearSVC` minimizes the squared hinge loss while `SVC` minimizes the regular hinge loss \n",
    "    - `LinearSVC` uses the one-vs-all (also known as one-vs-rest) multiclass reduction while `SVC` uses the one-vs-one multiclass reduction (this is important only when having > 2 classes!)\n",
    "- `svm.NuSVC()`, which is again very similar,\n",
    "    - Does have a \"kernel\" argument\n",
    "    - `SVC` and `NuSVC` are essentially the same thing, except that for `NuSVC`, `C` is reparametrized into `nu`. The advantage of this is that where `C` has no bounds and can be any positive number, `nu` always lies between 0 and 1  \n",
    "    - One-vs-one multiclass approach \n",
    "    \n",
    "    \n",
    "So what does one-vs-one mean? What does one-vs-all mean? \n",
    "\n",
    "- One-vs-one means that with $n$ classes, $\\dfrac{(n)*(n-1)}{2}$ boundaries are constructed! \n",
    "- One-vs-all means that when there are $n$ classes, $n$ boundaries are created.\n",
    "\n",
    "The difference between these three types of classifiers is mostly small but generally visible for datasets with 3+ classes. Have a look at our third example and see how the results differ!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Kernel Trick + Lab\n",
    "- idea behind [kernel](https://scikit-learn.org/stable/modules/svm.html#kernel-functions) methods is to create (nonlinear) combinations of the original features and project them onto a higher-dimensional space\n",
    "- some kernels have additional parameters that can be specified and knowing how these parameters work is critical to tuning SVMs\n",
    "- linear: represented by the inner product of the $\\langle x, x' \\rangle$\n",
    "- Radial Basis Function (RBF): $\\exp{(-\\gamma \\lVert x - x' \\rVert^2)} $\n",
    "    - The parameter $C$ is common to all SVM kernels. Again, by tuning the $C$ parameter when using kernels, you can provide a trade-off between misclassification of the training set and simplicity of the decision function. A high $C$ will classify as many samples correctly as possible (and might potentially lead to overfitting)\n",
    "    - gamma, $\\gamma$, defines how much influence a single training example has. The larger $\\gamma$ is, the closer other examples must be to be affected\n",
    "\n",
    "- polynomial: $(\\gamma \\langle x - x' \\rangle+r)^d $\n",
    "    - $d$ can be specified by the parameter degree. The default degree is 3.\n",
    "    - $r$ can be specified by the parameter coef0. The default is 0.\n",
    "- sigmoid: $\\tanh ( \\gamma\\langle x - x' \\rangle+r) $\n",
    "\n",
    "- NuSVC: similar to SVC, but adds an additional parameter, $\\nu$, which controls the number of support vectors and training errors. $\\nu$ jointly creates an upper bound on training errors and a lower bound on support vectors.\n",
    "    - Just like SVC, NuSVC implements the \"one-against-one\" approach when there are more than 2 classes. This means that when there are n classes, $\\dfrac{n*(n-1)}{2}$ classifiers are created, and each one classifies samples in 2 classes.\n",
    "\n",
    "- LinearSVC: similar to SVC, but instead of the \"one-versus-one\" method, a \"one-vs-rest\" method is used. So in this case, when there are $n$ classes, just $n$ classifiers are created, and each one classifies samples in 2 classes, the one of interest, and all the other classes. This means that SVC generates more classifiers, so in cases with many classes, LinearSVC actually tends to scale better.\n",
    "\n",
    "- probability=True to get probability score per class\n",
    "    - makes the calculations longer because cv needs to be done to calculate probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 32: ML Pipelines\n",
    "- Pipelines create an efficient workflow to combine data manipulations, preprocessing, and modeling\n",
    "\n",
    "[Pipelines](https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html)\n",
    "\n",
    "[Integrating Grid Search](https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines in sklearn Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the sequence of actions to perform\n",
    "pipe = Pipeline([('mms', MinMaxScaler()),\n",
    "                 ('tree', DecisionTreeClassifier(random_state=123))])\n",
    "# fit the model,\n",
    "pipe.fit(X_train, y_train)\n",
    "# score the model on test data\n",
    "pipe.score(X_test, y_test)\n",
    "\n",
    "# or implement GridSearch\n",
    "grid = [{'tree__max_depth': [None, 2, 6, 10], \n",
    "         'tree__min_samples_split': [5, 10]}]\n",
    "gridsearch = GridSearchCV(estimator=pipe, \n",
    "                          param_grid=grid, \n",
    "                          scoring='accuracy', \n",
    "                          cv=5)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "gridsearch.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeline with StandardScaler and KNeighborsClassifier\n",
    "scaled_pipeline_1 = Pipeline([('ss', StandardScaler()), \n",
    "                              ('knn', KNeighborsClassifier())])\n",
    "scaled_pipeline_1.fit(X_train, y_train)\n",
    "scaled_pipeline_1.score(X_test, y_test)\n",
    "\n",
    "scaled_pipeline_2 = Pipeline([('ss', StandardScaler()), \n",
    "                              ('RF', RandomForestClassifier(random_state=123))])\n",
    "grid = [{'RF__max_depth': [4, 5, 6], \n",
    "         'RF__min_samples_split': [2, 5, 10], \n",
    "         'RF__min_samples_leaf': [1, 3, 5]}]\n",
    "gridsearch = GridSearchCV(estimator=scaled_pipeline_2, \n",
    "                          param_grid=grid, \n",
    "                          scoring='accuracy', \n",
    "                          cv=5)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "gridsearch.score(X_test, y_test)\n",
    "gridsearch.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Notes\n",
    "[scoring parameters](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.boxplot([df[col] for col in df.columns])\n",
    "plt.title(\"Box plot of all columns in dataset\")\n",
    "plt.xticks(range(len(df.columns.values)), df.columns.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "227.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
