{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Topics 29 - 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 29: Decision Trees\n",
    "### Intro to Decision Trees\n",
    "- “recursive binary splitting\" The process of training a decision tree and predicting the target features of a dataset is as follows:\n",
    "    1. Present a dataset of training examples containing features/predictors and a target (similar to classifiers we have seen earlier).\n",
    "    2. Train the tree model by making splits for the target using the values of predictors. Which features to use as predictors gets selected following the idea of feature selection and uses measures like \"information gain\" and \"Gini Index\". We shall cover these shortly.\n",
    "    3. The tree is grown until some stopping criteria is achieved. This could be a set depth of the tree or any other similar measure.\n",
    "    4. Show a new set of features to the tree, with an unknown class and let the example propagate through a trained tree. The resulting leaf node represents the class prediction for this example datum.\n",
    "- CART (Classification and Regression Trees) uses the Gini Index as a metric\n",
    "- ID3 (Iterative Dichotomiser 3) uses the entropy function and information gain as metrics\n",
    "- $Entropy(p) = -\\sum (P_i . log_2(P_i))$\n",
    "- High entropy means less predictive power\n",
    "- As input, the function should take in `D` as a class distribution array for target class, and `a` the class distribution of the attribute to be tested, then calculate gain as $gain(D,A) = Entropy(D) - \\sum(\\frac{|D_i|}{|D|}.Entropy(D_i))$, where $D_{i}$ represents distribution of each class in `a`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Classification Trees: Perfect Split with Information Gain Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T23:43:43.661937Z",
     "start_time": "2021-02-05T23:43:43.655294Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "def entropy(pi):\n",
    "    \"\"\"\n",
    "    return the Entropy of a probability distribution:\n",
    "    entropy(p) = - SUM (Pi * log(Pi) )\n",
    "    pi is a list of how many occurances there are in each class\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for p in pi:\n",
    "        p = p / sum(pi)\n",
    "        if p != 0:\n",
    "            total +=  p * log(p, 2)\n",
    "        else:\n",
    "            total += 0\n",
    "    total *= -1\n",
    "    return total\n",
    "def IG(D, a):\n",
    "    \"\"\"\n",
    "    return the information gain:\n",
    "    gain(D, A) = entropy(D)− SUM( |Di| / |D| * entropy(Di) )\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for Di in a:\n",
    "        total += abs(sum(Di) / sum(D)) * entropy(Di)\n",
    "    gain = entropy(D) - total\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Trees using scikit-learn + Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import tree\n",
    "# One-hot encode the training data and show the resulting DataFrame with proper column names\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train)\n",
    "X_train_ohe = ohe.transform(X_train).toarray()\n",
    "# Creating this DataFrame is not necessary its only to show the result of the ohe\n",
    "ohe_df = pd.DataFrame(X_train_ohe, columns=ohe.get_feature_names(X_train.columns))\n",
    "ohe_df.head()\n",
    "# Create the classifier, fit it on the training data and make predictions on the test set\n",
    "clf = DecisionTreeClassifier(criterion='entropy') # or 'gini'\n",
    "clf.fit(X_train_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1,ncols = 1, figsize = (3,3), dpi=300)\n",
    "tree.plot_tree(clf,\n",
    "               feature_names = ohe_df.columns, \n",
    "               class_names=np.unique(y).astype('str'),\n",
    "               filled = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ohe = ohe.transform(X_test)\n",
    "y_preds = clf.predict(X_test_ohe)\n",
    "\n",
    "# Calculate accuracy \n",
    "acc = accuracy_score(y_test,y_pred) * 100\n",
    "print('Accuracy is :{0}'.format(acc))\n",
    "\n",
    "# Check the AUC for predictions\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print('\\nAUC is :{0}'.format(round(roc_auc, 2)))\n",
    "\n",
    "# Create and print a confusion matrix \n",
    "print('\\nConfusion Matrix')\n",
    "print('----------------')\n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "# or\n",
    "# Alternative confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(classifier, X, y, values_format='.3g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning and Pruning in Decision Trees + Lab\n",
    "We can prune our trees using:\n",
    "- Maximum depth: Reduce the depth of the tree to build a generalized tree. Set the depth of the tree to 3, 5, 10 depending after verification on test data\n",
    "- Minimum samples leaf with split: minimum number of samples required to split an internal node.\n",
    "- max_depth and min_samples_split are also both related to the computational cost\n",
    "- Minimum leaf sample size: minimum number of samples that we want a leaf node to contain. When this minimum size is achieved at a nodE. Size in terminal nodes can be fixed to 30, 100, 300 or 5% of total\n",
    "- Maximum leaf nodes: Reduce the number of leaf nodes\n",
    "- Maximum features: Maximum number of features to consider when splitting a node\n",
    "- The main difference between the two is that min_samples_leaf guarantees a minimum number of samples in a leaf, while min_samples_split can create arbitrary small leaves, though min_samples_split is more common in practice\n",
    "\n",
    "- For instance, if min_samples_split = 5, and there are 7 samples at an internal node, then the split is allowed. But let's say the split results in two leaves, one with 1 sample, and another with 6 samples. If min_samples_leaf = 2, then the split won't be allowed (even if the internal node has 7 samples) because one of the leaves resulted will have less than the minimum number of samples required to be at a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with CART Trees + Lab\n",
    "- Recursive partitioning, instead of global model.\n",
    "- Cost Function: $J(D, \\theta) = \\frac{n_{left}}{n_{total}} MSE_{left} + \\frac{n_{right}}{n_{total}} MSE_{right}$\n",
    "    - $D$: remaining training examples   \n",
    "    - $n_{total}$ : number of remaining training examples\n",
    "    - $\\theta = (f, t_f)$: feature and feature threshold\n",
    "    - $n_{left}/n_{right}$: number of samples in the left/right subset\n",
    "    - $MSE_{left}/MSE_{right}$: MSE of the left/right subset\n",
    "- $ \\hat{y}_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} y_i $\n",
    "$ MSE_m = \\frac{1}{n_{m}} \\sum_{i \\in D_m} (y_i - \\hat{y}_m)^2 $\n",
    "    - $D_m$: training examples in node $m$\n",
    "    - $n_{m}$ : total number of training examples in node $m$\n",
    "    - $y_i$: target value of $i-$th example\n",
    "- Without regularization, decision trees are likely to overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state=42, max_depth=3)\n",
    "regressor.fit(X_train, y_train)\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('MSE score:', mse(y_test, y_pred))\n",
    "print('R-sq score:', r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Trees and Model Optimization Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 30: Ensemble Methods\n",
    "\n",
    "- Bagging, short for Bootstrap Aggregation, is a combination of two ideas -- bootstrap resampling and aggregation\n",
    "- common approach is to treat each classifier in the ensemble's prediction as a \"vote\" and let our overall prediction be the majority vote\n",
    "- also common to see ensembles that take the arithmetic mean of all predictions, or compute a weighted average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "- classification and regression, ensemble of decision trees \n",
    "- Bagging: sample two-thirds of our training data with replacement\n",
    "- the algorithm then uses the remaining one-third of data that wasn't sampled to calculate the Out-Of-Bag Error\n",
    "- \n",
    "- Subspace sampling method: randomly select a subset of features (exact number is tunable parameter) to use as predictors for each node when training a decision tree\n",
    "- Benefits: Strong performance and interpretability \n",
    "- Drawbacks: Computational complexity and memory storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Ensembles and Random Forests Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV + Lab\n",
    "GridSearchCV: combines K-Fold Cross-Validation with a grid search of parameters\n",
    "- very time consuming and computationally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [1, 2, 5, 10],\n",
    "    'min_samples_split': [1, 5, 10, 20]\n",
    "}\n",
    "gs_tree = GridSearchCV(clf, param_grid, cv=3)\n",
    "gs_tree.fit(train_data, train_labels)\n",
    "gs_tree.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting and Weak Learners + Lab\n",
    "Boosting works as follows:\n",
    "1. Train a single weak learner\n",
    "2. Figure out which examples the weak learner got wrong\n",
    "3. Build another weak learner that focuses on the areas the first weak learner got wrong\n",
    "4. Continue this process until a predetermined stopping condition is met, such as until a set number of weak learners have been created, or the model's performance has plateaued\n",
    "\n",
    "Adaboost:\n",
    "- Trained on subset of training sample w/ replacement like bagging, except each data point carries a weight. Weight increases when weak longer misclassifies. Each weight acts as a probability that sample will go into bag\n",
    "\n",
    "Gradient Boosted Trees:\n",
    "- Starts with a weak learning and then calculates the Residuals for each data point.\n",
    "- Model combines the Residuals with a differentiable loss function to calculate the overall loss\n",
    "- Use gradients and the loss as predictors to train the next tree against. \n",
    "- Next learner focuses on these harder examples. Loss is reduced because these more commonly misclassified data points are focused on.\n",
    "- Gamma, learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "- parallelizes the construction of trees across all your computer's CPU cores during the training phase. It also allows for more advanced use cases\n",
    "- automatically handles missing values\n",
    "- `conda install py-xgboost`\n",
    "- https://xgboost.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 31: Support Vector Machines\n",
    "- Max Margin classifier: aim to maximize the margin\n",
    "    - $ b + w_Tx^{(i)} \\geq 1$ if $y ^{(i)} = 1$, OR $y ^{(i)} (b + w_Tx^{(i)} )\\geq 1$ for each $i$\n",
    "    - $w_T$ term is called the **weight vector**, $b$ term is called the **bias** \n",
    "- Soft Margin classifier:\n",
    "    - $ b + w_Tx^{(i)} \\geq 1-\\xi^{(i)}$ if $y ^{(i)} = 1$\n",
    "    - $\\xi^{(i)}$ is a **slack variable**\n",
    "- Hyperplane defined by weight vector wT and bias b\n",
    "- [kernels](https://scikit-learn.org/stable/modules/svm.html#kernel-functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an SVM using scikit-learn Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Kernel Trick + Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 32: ML Pipelines\n",
    "- Pipelines create an efficient workflow to combine data manipulations, preprocessing, and modeling\n",
    "\n",
    "[Pipelines](https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html)\n",
    "\n",
    "[Integrating Grid Search](https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines in sklearn Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the sequence of actions to perform\n",
    "pipe = Pipeline([('mms', MinMaxScaler()),\n",
    "                 ('tree', DecisionTreeClassifier(random_state=123))])\n",
    "# fit the model,\n",
    "pipe.fit(X_train, y_train)\n",
    "# score the model on test data\n",
    "pipe.score(X_test, y_test)\n",
    "\n",
    "# or implement GridSearch\n",
    "grid = [{'tree__max_depth': [None, 2, 6, 10], \n",
    "         'tree__min_samples_split': [5, 10]}]\n",
    "gridsearch = GridSearchCV(estimator=pipe, \n",
    "                          param_grid=grid, \n",
    "                          scoring='accuracy', \n",
    "                          cv=5)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "gridsearch.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeline with StandardScaler and KNeighborsClassifier\n",
    "scaled_pipeline_1 = Pipeline([('ss', StandardScaler()), \n",
    "                              ('knn', KNeighborsClassifier())])\n",
    "scaled_pipeline_1.fit(X_train, y_train)\n",
    "scaled_pipeline_1.score(X_test, y_test)\n",
    "\n",
    "scaled_pipeline_2 = Pipeline([('ss', StandardScaler()), \n",
    "                              ('RF', RandomForestClassifier(random_state=123))])\n",
    "grid = [{'RF__max_depth': [4, 5, 6], \n",
    "         'RF__min_samples_split': [2, 5, 10], \n",
    "         'RF__min_samples_leaf': [1, 3, 5]}]\n",
    "gridsearch = GridSearchCV(estimator=scaled_pipeline_2, \n",
    "                          param_grid=grid, \n",
    "                          scoring='accuracy', \n",
    "                          cv=5)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "gridsearch.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Notes\n",
    "[scoring parameters](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "227.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
