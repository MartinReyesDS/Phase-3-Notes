{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Topics 25 - 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 25: Intro to Log. Regression\n",
    "(Binary Classification)\n",
    "\n",
    "The sigmoid function\n",
    "  \n",
    "$S(x) = \\dfrac{1}{1+e^{-x}}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Intro to Supervised Learning\n",
    "- ML\n",
    "    - Supervised L. : Classification (Cls) and Regression (Reg)\n",
    "        - labeled training data\n",
    "            - labeling data is time-consuming and expensive, so AWS Mechanical Turk (MTurk) is typically used.\n",
    "            - we need enough negative examples (quantity and variety) to allow the model to learn\n",
    "        - objective/loss functions (performance)\n",
    "        - Cls (categorical): SVM, Discriminant Analysis, Naive Bayes, KNN, Logistic Regression, Trees\n",
    "            - Binary Cls\n",
    "                - Logistic Regression\n",
    "            - Multiclass Cls\n",
    "        - Reg (continuous): SVM, SVR, SPR, Ensemble Methods, nn's, Linear/Lasso/Ridge/Polynomial\n",
    "            - How much/many?\n",
    "            - Label is a real-valued number\n",
    "        - Decision Trees (continuous)\n",
    "        - Random Forests (continuous)\n",
    "    - Unsupervised L. : Clustering (Clu) and Association Analysis (AsAn) and Hidden Markov Model (HMM)\n",
    "        - unlabeled training data\n",
    "        - Clu (continuous): K-Means, K-Medroids, Fuzzy C-Means, Heirarchical, Gaussian Mixture, nn's, HMM, SVD, PCA\n",
    "        - AsAn (categorical): Apriori, FP_Growth\n",
    "        - HMM (categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear to Logistic regression\n",
    "\n",
    "- Logistic Regression Assumptions:\n",
    "    - Binary Log Reg requires the y to be binary with 1 representing the desired outcome.\n",
    "    - Only meaningful variables should be included (Regularization)\n",
    "    - little/no multicollinearity (_independent_ variables)\n",
    "    - independent variables are linearly related to the log odds\n",
    "    - requires quite large sample sizes\n",
    "\n",
    "- .fit() parameters:\n",
    "    - C : float, default=1.0\n",
    "        - Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "    - solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs'\n",
    "        -  Algorithm to use in the optimization problem\n",
    "        - 'liblinear' is limited to one-versus-rest schemes\n",
    "        - For small datasets, 'liblinear' is a good choice, whereas 'sag' and 'saga' are faster for large ones.\n",
    "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss; 'liblinear' is limited to one-versus-rest schemes.\n",
    "        - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
    "        - 'liblinear' and 'saga' also handle L1 penalty\n",
    "        - 'saga' also supports 'elasticnet' penalty\n",
    "        - 'liblinear' does not support setting ``penalty='none'``\n",
    "    - penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
    "        - Used to specify the norm used in the penalization. The 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is only supported by the 'saga' solver. If 'none' (not supported by the liblinear solver), no regularization is applied.\n",
    "        \n",
    "- outcome variable should be interpreted as the probability of the class label to be equal to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "regr = LogisticRegression(C=1e5, solver='liblinear')\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "regr.coef_\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "residuals = np.abs(y_train - y_hat_train) # or y train\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sms\n",
    "X = pd.get_dummies(salaries[['Race', 'Sex', 'Age']], drop_first=True, dtype=float)\n",
    "y = pd.get_dummies(salaries['Target'], drop_first=True, dtype=float)['>50K']\n",
    "# Create intercept term required for sm.Logit\n",
    "X = sms.add_constant(X)\n",
    "logit_model = sm.Logit(y, X)\n",
    "result = logit_model.fit()\n",
    "result.summary()\n",
    "np.exp(result.params) # e ^ coefficients \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices + Lab\n",
    "- used to evaluate classification models\n",
    "- predicted label on bottom (columns/x-axis)\n",
    "- actual label on left (rows/y-axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1]\n",
    "y_pred  = [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
    "confusion_matrix(y_true, y_pred, normalize)\n",
    "# Visualize your confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(logreg, X_test, y_test,\n",
    "                     cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics + Evaluating Logistic Regression Models Lab\n",
    "- quantify performance of classifiers\n",
    "- precision and recall have an inverse relationship\n",
    "   \n",
    "Precision: $ = \\frac{\\text{TP}}{\\text{TP + FP}} $\n",
    "- measures how precise the predictions are\n",
    "- how many of the selected things (Total predicted P) are relevant (TP)\n",
    "- \"Out of all the times the model said someone had a disease, how many times did the patient in question actually have the disease?\"\n",
    "- more conservative models can have a high precision score, but this doesn't necessarily mean that they are the best performing model\n",
    "\n",
    "Recall/Sensitivity: $ = \\frac{\\text{TP}}{\\text{TP + FN}} $\n",
    "- indicates what percentage of the classes we're interested in were actually captured by the model \n",
    "- how many of the relevant things (Actual Positives) are selected (TP)\n",
    "- Ex. â€œOut the patients that actually had the disease, what percentage did our model correctly identify as positive?\"\n",
    "- How good podel is at identifying positives\n",
    "\n",
    "Accuracy: $ = \\frac{\\text{TP + TN}}{\\text{TP + TN + FP + FN}} $\n",
    "- measures percentage of predictions a model gets right\n",
    "- \"Out of all the predictions our model made, what percentage were correct?\"\n",
    "\n",
    "F1 score: $ = 2\\ \\frac{Precision\\ x\\ Recall}{Precision + Recall} $\n",
    "- the Harmonic Mean of Precision and Recall, which means that the F1 score cannot be high without both precision and recall also being high. \"all around measure\"\n",
    "- penalizes models heavily if it skews too hard towards either precision or recall\n",
    "- generally the most used metric for describing the performance of a mode\n",
    "\n",
    "Specificity: $ = \\frac{\\text{TN}}{\\text{TN + FP}} $\n",
    "- what percent of total negatives were correctly identified\n",
    "- How good model is at predicting negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score,\n",
    "                                accuracy_score, f1_score\n",
    "print('Training Precision: ', precision_score(y_train, y_hat_train))\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('Training Recall: ', recall_score(y_train, y_hat_train))\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('Training Accuracy: ', accuracy_score(y_train, y_hat_train))\n",
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('Training F1-Score: ', f1_score(y_train, y_hat_train))\n",
    "print('Testing F1-Score: ', f1_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Precision: ', precision_score(y_train, y_hat_train))\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('Training Recall: ', recall_score(y_train, y_hat_train))\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('Training Accuracy: ', accuracy_score(y_train, y_hat_train))\n",
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('Training F1-Score: ', f1_score(y_train, y_hat_train))\n",
    "print('Testing F1-Score: ', f1_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves and AUC + Lab\n",
    "\n",
    "[Precision-Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve)\n",
    "\n",
    "[Visualizing/Plotting curves](https://scikit-learn.org/stable/visualizations.html#visualizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "# First calculate the probability scores of each of the datapoints:\n",
    "y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "print('ROC AUC: {}'.format(auc(fpr, tpr)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Problems\n",
    "\n",
    "- class_weight parameter in LogisticRegression(): None, 'balanced' or dict ({'class_label': weight})\n",
    "-  typically the heavier we weight the positive case, the better our classifier appears to be performing\n",
    "- Another technique, SMOTE (Synthetic Minority Oversampling): oversampling the minority class or undersampling the majority class\n",
    "    - Undersampling can only be used when you have a truly massive dataset and can afford to lose data points. However, even with very large datasets, you are losing potentially useful data. Oversampling can run into the issue of overfitting to certain characteristics of certain data points because there will be exact replicas of data points\n",
    "    - still maintain a test set from the original dataset in order to accurately judge the accuracy of the algorithm overall\n",
    "    - curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the level of class imbalance in the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Raw counts: \\n')\n",
    "print(df['target var'].value_counts())\n",
    "print('-----------------------------------')\n",
    "print('Normalized counts: \\n')\n",
    "print(df['target var'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T20:56:56.159905Z",
     "start_time": "2021-01-31T20:56:56.152206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now let's compare a few different regularization performances on the dataset:\n",
    "weights = [None, 'balanced', {1:2, 0:1}, {1:10, 0:1}, {1:100, 0:1}, {1:1000, 0:1}]\n",
    "names = ['None', 'Balanced', '2 to 1', '10 to 1', '100 to 1', '1000 to 1']\n",
    "colors = sns.color_palette('Set2')\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for n, weight in enumerate(weights):\n",
    "    # Fit a model with class_weights\n",
    "    logreg = LogisticRegression(fit_intercept=False, C=1e20, class_weight=weight, solver='lbfgs')\n",
    "    model_log = logreg.fit(X_train, y_train)\n",
    "    print(model_log\n",
    "          \n",
    "#     # Fit a model with SMOTE\n",
    "#     smote = SMOTE(sampling_strategy=ratio)\n",
    "#     X_train_resampled, y_train_resampled = smote.fit_sample(X_train, y_train) \n",
    "#     logreg = LogisticRegression(fit_intercept=False, C=1e20, solver ='lbfgs')\n",
    "#     model_log = logreg.fit(X_train_resampled, y_train_resampled)\n",
    "#     print(model_log)\n",
    "          \n",
    "#     # Fit a model with the C\n",
    "#     logreg = LogisticRegression(fit_intercept=False, C=c, solver='liblinear')\n",
    "#     model_log = logreg.fit(X_train, y_train)\n",
    "#     print(model_log) # Preview model params\n",
    "\n",
    "    # Predict\n",
    "    y_hat_test = logreg.predict(X_test)\n",
    "    y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score) \n",
    "    print('AUC for {}: {}'.format(names[n], auc(fpr, tpr)))\n",
    "    print('-------------------------------------------------------------------------------------')\n",
    "    lw = 2\n",
    "    # see how to plot ROC curve with sklearn\n",
    "    plt.plot(fpr, tpr, color=colors[n],\n",
    "             lw=lw, label='ROC curve {}'.format(names[n]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# Previous original class distribution\n",
    "print('Original class distribution: \\n')\n",
    "print(y.value_counts())\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_sample(X_train, y_train) \n",
    "# Preview synthetic sample class distribution\n",
    "print('-----------------------------------------')\n",
    "print('Synthetic sample class distribution: \\n')\n",
    "print(pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 26: MLE and Log. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE Review\n",
    "- maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations\n",
    "- probability vs likelihood\n",
    "    - probability = p(data | distribution parameters), area under curve\n",
    "    - likelihood = L(distribution parameters | data), y-value on the curve\n",
    "        - maximum likelihood finds the maximum y-value\n",
    "- When calculating maximum likelihood, it is common to use the log-likelihood, as taking the logarithm can simplify calculations.\n",
    "- MLE for binomial distribution:\n",
    "    - $L(p) = L(y_1, y_2, ..., y_n | p) = p^y (1-p)^{n-y}$ where $ y = \\sum_{i=1}^{n}y_i$\n",
    "    - $ln[L(p)] = ln[p^y (1-p)^{n-y}] = y ln(p)+(n-y)ln(1-p)$\n",
    "    - simplifies to : $p = \\frac{y}{n}$\n",
    "- [MLE for normal distribution](https://www.youtube.com/watch?v=Dn6b9fCIUpM)\n",
    "- [MLE for exponential distribution](https://www.youtube.com/watch?v=p3T-_LMrvBc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE and Logistic Regression\n",
    "\n",
    "- Mathematically, you can write each of these probabilities for each factor $X$ as:\n",
    "    - $\\pi_i = P(Y_i = 1|X_i = x_i)=\\dfrac{\\text{exp}(\\beta_0 + \\beta_1 x_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 x_i)}$\n",
    "\n",
    "- and maximize the likelihood function:\n",
    "    - $ L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Review\n",
    "- Recall that the general outline for gradient descent is:\n",
    "\n",
    "    - Define initial parameters:\n",
    "        - Pick a starting point\n",
    "        - Pick a step size $\\alpha$ (alpha)\n",
    "        - Choose a maximum number of iterations; the algorithm will terminate after this many iterations if a minimum has yet to be found\n",
    "        - (optionally) define a precision parameter; similar to the maximum number of iterations, this will terminate the algorithm early. For example, one might define a precision parameter of 0.00001, in which case if the change in the loss function were less then 0.00001, the algorithm would terminate. The idea is that we are very close to the bottom and further iterations would make a negligible difference\n",
    "    - Calculate the gradient at the current point (initially, the starting point)\n",
    "    - Take a step (of size alpha) in the direction of the gradient\n",
    "    - Repeat steps 2 and 3 until the maximum number of iterations is met, or the difference between two points is less then your precision parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 27: K Nearest Neighbors\n",
    "- [ex. KNN code from Medium](https://link.medium.com/Mvaj4jTpodb)\n",
    "- often used as a benchmark for more complex classifiers such as Artificial Neural Networks (ANN) and Support Vector Machines (SVM)\n",
    "- 1 < k < inf, but often k < 30\n",
    "- A very low k will fail to generalize (overfitting). A very high k is costly\n",
    "- lazy learner because it doesnâ€™t learn a discriminative function from the training data but memorizes the training dataset instead\n",
    "    - An eager learner has a model fitting or training step. A lazy learner does not have a training phase\n",
    "- pros:\n",
    "    - Quick to implement : Which is why it is popular as a benchmarking algorithm.\n",
    "    - Less training time: Faster turn around time\n",
    "    - Comparable accuracies: Its prediction accuracy as indicated in a lot of research papers is fairly high for a lot of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Metrics\n",
    "- Euclidean (L2) Distance: $ d_{L2}(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2} $, Most common distance metric\n",
    "- Chebyshev (Lâˆž) Distance: $ d_{Lâˆž}(x,y) = max(|x_1-x_2|,|y_1-y_2|)$\n",
    "- Manhattan ((L1) Distance: $ d_{L1}(x,y) = \\sum_{i=1}^{n}|x_i - y_i | $, Sum of the (absolute) differences of their coordinates.\n",
    "- Minkowski (Lc) Distance: $ d_{Lc}(x, y) = \\left(\\sum_{i=1}^{n}|x_i - y_i|^c\\right)^\\frac{1}{c}$\n",
    "    - np.power( np.sum( np.abs( np.array(a) + np.array(b)...) ) ** c, 1/c )\n",
    "---\n",
    "- \"distance quantifies similarity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors + Lab\n",
    "\n",
    "- normalize X data before fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform/normalize data (code below)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "test_preds = clf.predict(X_test)\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "def print_metrics(labels, preds):\n",
    "    print(\"Precision Score: {}\".format(precision_score(labels, preds)))\n",
    "    print(\"Recall Score: {}\".format(recall_score(labels, preds)))\n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(labels, preds)))\n",
    "    print(\"F1 Score: {}\".format(f1_score(labels, preds)))\n",
    "print_metrics(y_test, test_preds)\n",
    "def find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25):\n",
    "    best_k = 0\n",
    "    best_score = 0.0\n",
    "    for k in range(min_k, max_k+1, 2):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        preds = knn.predict(X_test)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        if f1 > best_score:\n",
    "            best_k = k\n",
    "            best_score = f1\n",
    "    print(\"Best Value for k: {}\".format(best_k))\n",
    "    print(\"F1-Score: {}\".format(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 28: Bayes Classification\n",
    "[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers with Bayes\n",
    "\n",
    "[Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "\n",
    "- Bayes Theorem: $P(Y|X_1, X_2, ..., X_n) $\n",
    "- Expanding to multiple features, the multinomial Bayes' formula is:\n",
    "$ P(y|x_1, x_2, ..., x_n) = \\dfrac{P(y) \\cdot P(x_1|y) \\cdot P(x_2|y) \\cdot ... \\cdot P(x_n|y)}{P(x_1, x_2, ..., x_n)} = \\dfrac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}$\n",
    "- assume that the features are independent of one another (naive assumption), estimate an overall probability by multiplying the conditional probabilities for each of the independent features, and make a classification prediction by seeing which probability is higher\n",
    "- since the denominator, $P(X_1, X_2, ..., X_n)$, is equal for both $P(Y_0)$ and $P(Y_1)$, you can compare the numerators, as these will be proportional to the overall probability.\n",
    "- Decision Rule for Bernoulli naive Bayes: $ P(x_i|y) = P(i | y)x_i + (1 - P(i | y))(1-x_i)$\n",
    "- Probability of category *t* in feature  given class  is estimated as: $ P(x_i|y) = \\dfrac{N_tic + \\alpha}{N_c + \\alpha n_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "clf = BernoulliNB()\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "clf.score(X_test, y_test) # Mean accuracy of self.predict(X) wrt. y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes + Lab\n",
    "\n",
    "- $ P(x_i|y) = \\dfrac{1}{\\sqrt{2\\pi \\sigma_i^2}}e^{\\frac{-(x-\\mu_i)^2}{2\\sigma_i^2}}$\n",
    "\n",
    "- $P(y|x_1,x_2,...x_n) = \\dfrac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1,x_2,...x_n)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target'].value_counts() # examine the target variable\n",
    "aggs = df.groupby('Target').agg(['mean', 'std'])\n",
    "aggs\n",
    "# Calculate conditional probability point estimates\n",
    "from scipy import stats\n",
    "def p_x_given_class(obs_row, feature, class_):\n",
    "    mu = aggs[feature]['mean'][class_] # mean of feature\n",
    "    std = aggs[feature]['std'][class_] # std of feature\n",
    "    \n",
    "    # A single observation\n",
    "    obs = df.iloc[obs_row][feature] # row to observe\n",
    "    \n",
    "    p_x_given_y = stats.norm.pdf(obs, loc=mu, scale=std)\n",
    "    return p_x_given_y\n",
    "# Multinomial Bayes\n",
    "# Calculating class probabilities for observations\n",
    "def predict_class(row):\n",
    "    c_probs = []\n",
    "    for c in range(3):\n",
    "        # Initialize probability to relative probability of class\n",
    "        p = len(df[df['Target'] == c])/len(df) # p(y)\n",
    "        for feature in X.columns:   # product of all p(xi|y)\n",
    "            p *= p_x_given_class(row, feature, c)\n",
    "        # Update the probability using the point estimate for each feature\n",
    "        c_probs.append(p)\n",
    "    return np.argmax(c_probs) # class with max of p(y|xi)\n",
    "# Calculating accuracy\n",
    "y_hat_train = [predict_class(X_train.iloc[idx]) for idx in range(len(X_train))]\n",
    "y_hat_test = [predict_class(X_test.iloc[idx]) for idx in range(len(X_test))]\n",
    "residuals_train = y_hat_train == y_train\n",
    "acc_train = residuals_train.sum()/len(residuals_train)\n",
    "residuals_test = y_hat_test == y_test\n",
    "acc_test = residuals_test.sum()/len(residuals_test)\n",
    "print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to lab for classication under a range of a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Classification with Naive Bayes + Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Notes\n",
    "\n",
    "- Ex. of scaling with StandardScaler\n",
    "    - only training data is fit_transformed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data_train = scaler.fit_transform(X_train) # np array\n",
    "scaled_data_test = scaler.transform(X_test) # np array\n",
    "scaled_df_train = pd.DataFrame(scaled_data_train, columns=one_hot_df.columns)\n",
    "scaled_df_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Sex to binary encoding\n",
    "df['Sex'] = df['Sex'].map({'female': 0, 'male': 1})\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "227.273px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
