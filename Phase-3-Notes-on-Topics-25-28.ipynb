{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Topics 25 - 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 25: Intro to Log. Regression\n",
    "(Binary Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Intro to Supervised Learning\n",
    "- ML\n",
    "    - Supervised L. : Classification (Cls) and Regression (Reg)\n",
    "        - labeled training data\n",
    "            - labeling data is time-consuming and expensive, so AWS Mechanical Turk (MTurk) is typically used.\n",
    "            - we need enough negative examples (quantity and variety) to allow the model to learn\n",
    "        - objective/loss functions (performance)\n",
    "        - Cls (categorical): SVM, Discriminant Analysis, Naive Bayes, KNN, Logistic Regression, Trees\n",
    "            - Binary Cls\n",
    "                - Logistic Regression\n",
    "            - Multiclass Cls\n",
    "        - Reg (continuous): SVM, SVR, SPR, Ensemble Methods, nn's, Linear/Lasso/Ridge/Polynomial\n",
    "            - How much/many?\n",
    "            - Label is a real-valued number\n",
    "        - Decision Trees (continuous)\n",
    "        - Random Forests (continuous)\n",
    "    - Unsupervised L. : Clustering (Clu) and Association Analysis (AsAn) and Hidden Markov Model (HMM)\n",
    "        - unlabeled training data\n",
    "        - Clu (continuous): K-Means, K-Medroids, Fuzzy C-Means, Heirarchical, Gaussian Mixture, nn's, HMM, SVD, PCA\n",
    "        - AsAn (categorical): Apriori, FP_Growth\n",
    "        - HMM (categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear to Logistic regression\n",
    "\n",
    "- Logistic Regression Assumptions:\n",
    "    - Binary Log Reg requires the y to be binary with 1 representing the desired outcome.\n",
    "    - Only meaningful variables should be included (Regularization)\n",
    "    - little/no multicollinearity (_independent_ variables)\n",
    "    - independent variables are linearly related to the log odds\n",
    "    - requires quite large sample sizes\n",
    "\n",
    "- .fit() parameters:\n",
    "    - C : float, default=1.0\n",
    "        - Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "    - solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs'\n",
    "        -  Algorithm to use in the optimization problem\n",
    "        - 'liblinear' is limited to one-versus-rest schemes\n",
    "        - For small datasets, 'liblinear' is a good choice, whereas 'sag' and 'saga' are faster for large ones.\n",
    "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss; 'liblinear' is limited to one-versus-rest schemes.\n",
    "        - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
    "        - 'liblinear' and 'saga' also handle L1 penalty\n",
    "        - 'saga' also supports 'elasticnet' penalty\n",
    "        - 'liblinear' does not support setting ``penalty='none'``\n",
    "    - penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
    "        - Used to specify the norm used in the penalization. The 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is only supported by the 'saga' solver. If 'none' (not supported by the liblinear solver), no regularization is applied.\n",
    "        \n",
    "- outcome variable should be interpreted as the probability of the class label to be equal to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "regr = LogisticRegression(C=1e5, solver='liblinear')\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "regr.coef_\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "residuals = np.abs(y_train - y_hat_train) # or y train\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import statsmodels.api as sms\n",
    "X = pd.get_dummies(salaries[['Race', 'Sex', 'Age']], drop_first=True, dtype=float)\n",
    "y = pd.get_dummies(salaries['Target'], drop_first=True, dtype=float)['>50K']\n",
    "# Create intercept term required for sm.Logit\n",
    "X = sms.add_constant(X)\n",
    "logit_model = sm.Logit(y, X)\n",
    "result = logit_model.fit()\n",
    "result.summary()\n",
    "np.exp(result.params) # e ^ coefficients \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices + Lab\n",
    "- used to evaluate classification models\n",
    "- predicted label on bottom (columns/x-axis)\n",
    "- actual label on left (rows/y-axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1]\n",
    "y_pred  = [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
    "confusion_matrix(y_true, y_pred, normalize)\n",
    "# Visualize your confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(logreg, X_test, y_test,\n",
    "                     cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics + Evaluating Logistic Regression Models Lab\n",
    "- quantify performance of classifiers\n",
    "- precision and recall have an inverse relationship\n",
    "   \n",
    "Precision: $ = \\frac{\\text{TP}}{\\text{TP + FP}} $\n",
    "- measures how precise the predictions are\n",
    "- how many of the selected things (Total predicted P) are relevant (TP)\n",
    "- \"Out of all the times the model said someone had a disease, how many times did the patient in question actually have the disease?\"\n",
    "- more conservative models can have a high precision score, but this doesn't necessarily mean that they are the best performing model\n",
    "\n",
    "Recall/Sensitivity: $ = \\frac{\\text{TP}}{\\text{TP + FN}} $\n",
    "- indicates what percentage of the classes we're interested in were actually captured by the model \n",
    "- how many of the relevant things (Actual Positives) are selected (TP)\n",
    "- Ex. “Out the patients that actually had the disease, what percentage did our model correctly identify as positive?\"\n",
    "- How good podel is at identifying positives\n",
    "\n",
    "Accuracy: $ = \\frac{\\text{TP + TN}}{\\text{TP + TN + FP + FN}} $\n",
    "- measures percentage of predictions a model gets right\n",
    "- \"Out of all the predictions our model made, what percentage were correct?\"\n",
    "\n",
    "F1 score: $ = 2\\ \\frac{Precision\\ x\\ Recall}{Precision + Recall} $\n",
    "- the Harmonic Mean of Precision and Recall, which means that the F1 score cannot be high without both precision and recall also being high. \"all around measure\"\n",
    "- penalizes models heavily if it skews too hard towards either precision or recall\n",
    "- generally the most used metric for describing the performance of a mode\n",
    "\n",
    "Specificity: $ = \\frac{\\text{TN}}{\\text{TN + FP}} $\n",
    "- what percent of total negatives were correctly identified\n",
    "- How good model is at predicting negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score,\n",
    "                                accuracy_score, f1_score\n",
    "print('Training Precision: ', precision_score(y_train, y_hat_train))\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('Training Recall: ', recall_score(y_train, y_hat_train))\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('Training Accuracy: ', accuracy_score(y_train, y_hat_train))\n",
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('Training F1-Score: ', f1_score(y_train, y_hat_train))\n",
    "print('Testing F1-Score: ', f1_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Precision: ', precision_score(y_train, y_hat_train))\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('Training Recall: ', recall_score(y_train, y_hat_train))\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('Training Accuracy: ', accuracy_score(y_train, y_hat_train))\n",
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('Training F1-Score: ', f1_score(y_train, y_hat_train))\n",
    "print('Testing F1-Score: ', f1_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves and AUC + Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "# First calculate the probability scores of each of the datapoints:\n",
    "y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "# can plot ROC curve with code at the end of the lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 26: MLE and Log. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE Review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE and Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 27: K Nearest Neighbors\n",
    "- [ex. KNN code from Medium](https://link.medium.com/Mvaj4jTpodb)\n",
    "- often used as a benchmark for more complex classifiers such as Artificial Neural Networks (ANN) and Support Vector Machines (SVM)\n",
    "- 1 < k < inf, but often k < 30\n",
    "- A very low k will fail to generalize (overfitting). A very high k is costly\n",
    "- lazy learner because it doesn’t learn a discriminative function from the training data but memorizes the training dataset instead\n",
    "    - An eager learner has a model fitting or training step. A lazy learner does not have a training phase\n",
    "- pros:\n",
    "    - Quick to implement : Which is why it is popular as a benchmarking algorithm.\n",
    "    - Less training time: Faster turn around time\n",
    "    - Comparable accuracies: Its prediction accuracy as indicated in a lot of research papers is fairly high for a lot of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Metrics\n",
    "- Euclidean (L2) Distance: $ d_{L2}(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2} $, Most common distance metric\n",
    "- Chebyshev (L∞) Distance: $ d_{L∞}(x,y) = max(|x_1-x_2|,|y_1-y_2|)$\n",
    "- Manhattan ((L1) Distance: $ d_{L1}(x,y) = \\sum_{i=1}^{n}|x_i - y_i | $, Sum of the (absolute) differences of their coordinates.\n",
    "- Minkowski (Lc) Distance: $ d_{Lc}(x, y) = \\left(\\sum_{i=1}^{n}|x_i - y_i|^c\\right)^\\frac{1}{c}$\n",
    "    - np.power( np.sum( np.abs( np.array(a) + np.array(b)...) ) ** c, 1/c )\n",
    "---\n",
    "- \"distance quantifies similarity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors + Lab\n",
    "\n",
    "- normalize X data before fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform/normalize data (code below)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(scaled_X_train, y_train)\n",
    "test_preds = clf.predict(scaled_data_test)\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "def print_metrics(labels, preds):\n",
    "    print(\"Precision Score: {}\".format(precision_score(labels, preds)))\n",
    "    print(\"Recall Score: {}\".format(recall_score(labels, preds)))\n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(labels, preds)))\n",
    "    print(\"F1 Score: {}\".format(f1_score(labels, preds)))\n",
    "print_metrics(y_test, test_preds)\n",
    "def find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25):\n",
    "    best_k = 0\n",
    "    best_score = 0.0\n",
    "    for k in range(min_k, max_k+1, 2):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        preds = knn.predict(X_test)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        if f1 > best_score:\n",
    "            best_k = k\n",
    "            best_score = f1\n",
    "    print(\"Best Value for k: {}\".format(best_k))\n",
    "    print(\"F1-Score: {}\".format(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 28: Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Notes\n",
    "\n",
    "- Ex. of scaling with StandardScaler\n",
    "    - only training data is fit_transformed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data_train = scaler.fit_transform(X_train) # np array\n",
    "scaled_data_test = scaler.transform(X_test) # np array\n",
    "scaled_df_train = pd.DataFrame(scaled_data_train, columns=one_hot_df.columns)\n",
    "scaled_df_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Sex to binary encoding\n",
    "df['Sex'] = df['Sex'].map({'female': 0, 'male': 1})\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "227.273px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
