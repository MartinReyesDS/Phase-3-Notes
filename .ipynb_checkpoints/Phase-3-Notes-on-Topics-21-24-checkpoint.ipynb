{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Topics 21 - 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 21: OOP + Appendix: More OOP\n",
    "- Classes should follow CamelCase convention\n",
    "- search order (method resolution order): instance, class, superclass(es)\n",
    "\n",
    "#### Intro to OOP - Crash Course\n",
    "- Four main principles: \n",
    "    - Encapsulation (information hiding):\n",
    "        - refers to bundling data with methods that can operate on that data within a class\n",
    "        - idea of hiding data within a class, preventing anything outside that class from directly interacting with it (e.g. edit its attributes)\n",
    "        - members of other classes can interact with attrs of another object through its methods\n",
    "        - keeps programmer in control of access to data and prevents program from ending up in any strange or unwanted states\n",
    "    - Abstraction: \n",
    "        - refers to only showing essential details and hiding everything else\n",
    "        - users of the class should not worry about inner workings/details of the class\n",
    "        - interface: the way sections of code can communicate with one another\n",
    "            - typically done through (getter) methods that each class can access\n",
    "        - implementation: implementation of the methods, or the method code, should be hidden\n",
    "        - prevent classes from becoming entangled, or else changes in one class will have a ripple effect on the others\n",
    "        - allows program to be worked on incrementally, prevents entanglement, and reduces complexity\n",
    "    - Inheritance: \n",
    "        - derive classes from other classes\n",
    "        - Access modifiers:\n",
    "            - Public: members/classes can be access from anywhere in your program\n",
    "            - Private: members can only be accessed within the same class that member is defined\n",
    "            - Protected: members can be accessed within the class it is defined, as well as its subclasses\n",
    "    - Polymorphism\n",
    "        - methods can take on many forms\n",
    "        - dynamic polymorphism\n",
    "            - occurs during runtime\n",
    "            - method signature is in both a subclass and a superclass, subclass overrides\n",
    "            - methods share same name/parameters but have different implementation\n",
    "        - static polymorphism (method overloading)\n",
    "            - occurs during compile time\n",
    "            - methods with the same name but different arguments (different number, type or order of parameters) are defined in the same class\n",
    "- Getting vs Setting vs Deleting Methods:\n",
    "    - set with property decorator\n",
    "    - Getting methods: get info from an object (attr/var could only be referenced, not changed \\[read-only\\])\n",
    "    - Setting methods: set attrs to different values\n",
    "    \n",
    "#### Appendix\n",
    "- Domain Model\n",
    "- Abstract super classes (class AbsCls(object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T20:12:32.094678Z",
     "start_time": "2021-01-19T20:12:32.087984Z"
    }
   },
   "outputs": [],
   "source": [
    "class Employee:                                             # defining class\n",
    "    \n",
    "    num_of_emps = 0\n",
    "    raise_amount = 1.04                                     # class variable\n",
    "    \n",
    "    def __init__(self, first, last, pay):                   # constructor\n",
    "        self.first = first                                  # instance attribute (variable defined in constructor)\n",
    "        self.last = last\n",
    "        self.pay = pay\n",
    "        Employee.num_of_emps += 1\n",
    "    \n",
    "    @property                                               # getter method\n",
    "    def email(self):                                        # defining email attribute in the class like it is a method\n",
    "        return '{}.{}@email.com'.format(self.first, self.last)\n",
    "    \n",
    "    @property\n",
    "    def fullname(self):                                  \n",
    "        return '{} {}'.format(self.first, self.last)\n",
    "    \n",
    "    @fullname.setter                                        # setter method\n",
    "    def fullname(self, name):                               # executed when emp_1.fullname is assigned a string\n",
    "        first, last = name.split(' ')\n",
    "        self.first = first\n",
    "        self.last = last\n",
    "    \n",
    "    @fullname.deleter                                       # deleter method (executed of we run 'del emp_1.fullname)\n",
    "    def fullname(self):\n",
    "        self.first = None\n",
    "        self.last = None\n",
    "\n",
    "    def apply_raise(self):                                  # instance method (function defined within the class)\n",
    "        self.pay = int(self.pay * self.raise_amount)\n",
    "        \n",
    "    @classmethod                                            # class method decorator\n",
    "    def set_raise_amt(cls, amount):                         # class method, can use as alternative constructor (seen in datetime module)\n",
    "        cls.raise_amt = amount\n",
    "    \n",
    "    @staticmethod                                           # static method decorator\n",
    "    def is_workday(day):                                    # static method, doesn't auto pass in instance or class\n",
    "        if day.weekday() == 5 or day.weekday() == 6:\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "class Manager(Employee):                                    # subclass(inherited superclass)\n",
    "    \n",
    "    def __init__(self, first, last, pay, employees=None):   # never set mutable data types as default arguments\n",
    "        super().__init__(first, last, pay)\n",
    "        if employees is None:\n",
    "            self.employees = []\n",
    "        else:\n",
    "            self.employees = employees\n",
    "            \n",
    "    def add_emp(self, emp):\n",
    "        if emp not in self.employees:\n",
    "            self.employees.append(emp)\n",
    "\n",
    "    def remove_emp(self, emp):\n",
    "        if emp in self.employees:\n",
    "            self.employees.remove(emp)\n",
    "\n",
    "emp_1 = Employee('Corey','Schafer', 50000) # instance instantiated\n",
    "emp_1.first = 'corey' # instance variable set outside of the constructor\n",
    "Employee.set_raise_amt(1.05)\n",
    "\n",
    "print(Employee.__dict__) # class variables and methods\n",
    "print(emp_1.__dict__) # instance variables\n",
    "\n",
    "Employee.set_raise_amt(1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special (Magic/Dunder) Methods on bottom of page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>\n",
    "\n",
    "\n",
    "## Topic 22: Linear Algebra + Appendix: More Lin. Alg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systems of linear equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T20:54:22.895401Z",
     "start_time": "2021-01-19T20:54:22.859953Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    q + d = 30\n",
    "    .25q + .1d = 5.7\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "A = np.array([[  1, 1],\n",
    "              [.25,.1]])\n",
    "s = np.array([[30],[5.7]])\n",
    "la.solve(A,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalars, Vectors, Matrices, and Tensors\n",
    "- vector: 1-D tensor, matrix: 2-D tensor\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T21:06:26.407747Z",
     "start_time": "2021-01-19T21:06:26.359617Z"
    }
   },
   "outputs": [],
   "source": [
    "# numpy methods/attrs\n",
    "np.array([[,][,]]) # array (vector/matrix)\n",
    "x = np.linspace(start, stop, num) # generate vector of 'num' samples between 'start' and 'stop'\n",
    "# x[int/ -int /:] # indexing, get 'int'-th element\n",
    "# x[::-1] # reverse the vector\n",
    "# x[0::2]  # every other element\n",
    "np.asmatrix(A) # or np.mat(A), returns matrix([[,],[,]]) instead of array\n",
    "np.mat(A) # does not make a copy if the input is already a matrix or an ndarray.\n",
    "          # Equivalent to np.matrix(data, copy=False)\n",
    "# X[:,:]  index or assign new value\n",
    "# .shape or np.shape(A)\n",
    "# X.T or np.transpose(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n",
    "- element-wise multiplication: matrices must be same shape\n",
    "- dot product: Anm dot Bmo = Cno\n",
    "- cross product: $a \\times b = \\mid a \\mid  \\mid b \\mid \\sin(\\theta) n $\n",
    "     - $\\mid a \\mid$  is the magnitude (length) of vector $a$\n",
    "     - $\\mid b \\mid$  is the magnitude (length) of vector $b$\n",
    "     - $\\theta$ is the angle between $a$ and $b$\n",
    "     - $n$ is the unit vector at right angles to both $a$ and $b$\n",
    "     - $a \\times b = - a \\times b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A * B # element-wise/Hadamard Product\n",
    "A.dot(B) # or\n",
    "np.dot(A, B) # dot product\n",
    "bp.cross(a, b) # cross product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Systems of Linear Equations with NumPy\n",
    "- $A \\cdot A^{-1} = I$\n",
    "- **($A \\cdot X = B$)**  -->  ($ A^{-1} \\cdot A \\cdot X = A^{-1} \\cdot B$)  -->  ($I \\cdot X = A^{-1} \\cdot B$)  -->  **($X = A^{-1} \\cdot B$)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T21:50:08.143560Z",
     "start_time": "2021-01-19T21:50:08.139793Z"
    }
   },
   "outputs": [],
   "source": [
    "np.zeros/ones(r, c)\n",
    "np.eye(4) # or\n",
    "np.identity(4, dtype=int)\n",
    "la.inv(A)\n",
    "np.matrix.round(A)\n",
    "la.solve(A,s)\n",
    "np.square/sqrt(x)\n",
    "np.arccos/sin/tan(x) # in radians\n",
    "np.random.rand(r,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Analysis using Linear Algebra and NumPy\n",
    "$$\n",
    "    \\left[ {\\begin{array}{cc}\n",
    "   1 & 1 \\\\\n",
    "   1 & 2 \\\\\n",
    "   1 & 3 \\\\\n",
    "  \\end{array} } \\right]\n",
    "   \\left[ {\\begin{array}{c}\n",
    "   c \\\\\n",
    "   m \\\\\n",
    "  \\end{array} } \\right] =\n",
    "    \\left[ {\\begin{array}{c}\n",
    "    1 \\\\\n",
    "    2 \\\\\n",
    "    2 \\\\\n",
    "  \\end{array} } \\right] \n",
    "$$\n",
    "\n",
    "If you don't include this constant (column of ones), then the function is constrained to the origin (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T23:37:44.069300Z",
     "start_time": "2021-01-19T23:37:44.064984Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.polynomial.polynomial import polyfit # least squares polynomial fit\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, m = polyfit(x, y, 1) # 1 is degree, c(intercept) and m(slope)\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, c + (m * x), '-')\n",
    "plt.xticks(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary least squares \n",
    "\n",
    "A common measure to find and minimize the value of this error is called *Ordinary Least Squares*. \n",
    "\n",
    "This says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\n",
    "\n",
    "In matrix terms, the same equation can be written as:\n",
    "\n",
    "$ y = \\boldsymbol{X} b + e $\n",
    "\n",
    "This says to get y, multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of **1**s in it for the intercept. For each day, the **1** is used to add the intercept in the first row of the column vector $b$.\n",
    "\n",
    "Let's assume that the error is equal to zero on average and drop it to sketch a proof:\n",
    "\n",
    "$ y = \\boldsymbol{X} b$\n",
    "\n",
    "\n",
    "Now let's solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n",
    "\n",
    "$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\n",
    "\n",
    "And now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n",
    "\n",
    "$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\n",
    "\n",
    "\n",
    "$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\n",
    "\n",
    "$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\n",
    "\n",
    "$ b= \\hat{X} $\n",
    "\n",
    "With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (\"$X$-hat\"), you need to solve the above equation. \n",
    "\n",
    "Remember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you're looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate an OLS Regression Line\n",
    "X = np.array([[1, 1],[1, 2],[1, 3]])\n",
    "y = np.array([1, 2, 2])\n",
    "Xt = X.T\n",
    "XtX = Xt.dot(X)\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "Xty = Xt.dot(y)\n",
    "x_hat = XtX_inv.dot(Xty) # the value for b shown above\n",
    "x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting regression line\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, x_hat[0] + (x_hat[1] * x), '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observed data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\n",
    "\n",
    "predicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\n",
    "\n",
    "error $\\rightarrow$ $\\epsilon = y - \\hat y $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Analysis using Linear Algebra and NumPy Lab\n",
    "1. Prepare data for modeling\n",
    "2. Perform a 80/20 t-t-split\n",
    "3. Calculate the beta, $\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n",
    "4. Make predictions\n",
    "5. Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity: From OLS to Gradient Descent\n",
    "- The Big O Notation:\n",
    "    - $O(\\log n)$: aka $\\log$ time\n",
    "    - $O(n)$: aka linear time\n",
    "    - $O(n^2)$: quadratic\n",
    "    - $O(n^3)$: cubic\n",
    "        - inverting a matrix\n",
    "- OLS linear regression is computed as: $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y$.\n",
    "\n",
    "    - If $\\boldsymbol{X}$ is an $(n \\times k)$ matrix:\n",
    "\n",
    "    - $(\\boldsymbol{X}^T\\boldsymbol{X})$ takes $O(n*k^2)$ time and produces a $(k \\times k)$ matrix\n",
    "    - The matrix inversion of a (k x k) matrix takes $O(k^3)$ time\n",
    "$(\\boldsymbol{X}^T\\boldsymbol{y})$ takes $O(n*k^2)$ time and produces a $(k \\times k)$ matrix\n",
    "    - The final matrix multiplication of two $(k \\times k)$ matrices takes $O(k^3)$ time\n",
    "    \n",
    "**So the Big O running time for OLS is $O(k^{2*(n + k)})$ - which is pretty expensive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Vector Addition and Broadcasting in NumPy\n",
    "- you can add/subtract (element-wise or broadcasting) vectors and matrices\n",
    "- Broadcasting Analogy: scalar + vector : vector + matrix\n",
    "\n",
    "### Appendix: Properties of Dot Product\n",
    "- Distributive Property - matrix multiplication IS distributive\n",
    "    - $A \\cdot (B+C) = (A \\cdot B + A \\cdot C) $\n",
    "- Associative Property - matrix multiplication IS associative\n",
    "    - $A \\cdot (B \\cdot C) = (A \\cdot B) \\cdot C $\n",
    "- Commutative Property - matrix multiplication is NOT commutative\n",
    "    - $A \\cdot B \\neq B \\cdot A $\n",
    "- Commutative Property - vector multiplication IS commutative\n",
    "    - $x^T \\cdot y = y^T \\cdot x$\n",
    "- Simplification of the matrix product\n",
    "    - $ (A \\cdot B)^T = B^T \\cdot A^T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>\n",
    "\n",
    "\n",
    "## Topic 23: Calculus, Cost Function, and Gradient Descent + Appendix: More on Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Derivatives\n",
    "$ f'(x) = \\lim_{ h\\to0} \\frac{f(x + h) - f(x)}{h} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T02:59:10.559614Z",
     "start_time": "2021-01-20T02:59:10.552363Z"
    }
   },
   "outputs": [],
   "source": [
    "def term_output(array, input_value):\n",
    "    return array[0]*input_value**array[1]\n",
    "# ex. term_output(np.array([3, 2]), 2)\n",
    "def output_at(array_of_terms, x_value):\n",
    "    outputs = []\n",
    "    for i in range(int(np.shape(array_of_terms)[0])):\n",
    "        outputs.append(array_of_terms[i][0]*x_value**array_of_terms[i][1])\n",
    "    return sum(outputs)\n",
    "def delta_f(array_of_terms, x_value, delta_x):\n",
    "    return output_at(array_of_terms, x_value + delta_x) - output_at(array_of_terms, x_value)\n",
    "def derivative_of(array_of_terms, x_value, delta_x):\n",
    "    delta = delta_f(array_of_terms, x_value, delta_x)\n",
    "    return round(delta/delta_x, 3)\n",
    "def tangent_line(array_of_terms, x_value, line_length = 4, delta_x = .01):\n",
    "    y = output_at(array_of_terms, x_value)\n",
    "    derivative_at = derivative_of(array_of_terms, x_value, delta_x)    \n",
    "    x_dev = np.linspace(x_value - line_length/2, x_value + line_length/2, 50)\n",
    "    tan = y + derivative_at *(x_dev - x_value)\n",
    "    return {'x_dev':x_dev, 'tan':tan, 'lab': \" f' (x) = \" + str(derivative_at)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_function = np.array([[4, 1], [15, 0]])\n",
    "x_values = np.linspace(0, 5, 100)\n",
    "y_values = [output_at(lin_function, x) for x in x_values]\n",
    "plt.plot(x_values, y_values, label = \"4x + 15\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives of Non-Linear Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(delta_a):\n",
    "    lab= \"delta x = \" + str(delta_a)\n",
    "    plt.plot(x, f(x), label = lab)\n",
    "    plt.hlines(y=9, xmin=1, xmax=3, linestyle = \"dashed\", color= 'lightgrey')\n",
    "    plt.vlines(x=2, ymin=1, ymax=4, linestyle = \"dashed\", color= 'lightgrey')\n",
    "    plt.hlines(y=4, xmin=1, xmax=2, linestyle = \"dashed\", color= 'lightgrey')\n",
    "    plt.vlines(x=3, ymin=1, ymax=9, linestyle = \"dashed\", color= 'lightgrey')\n",
    "    # tangent line\n",
    "    x_dev = np.linspace(1.5, 3.2, 100)\n",
    "    a = 2\n",
    "    fprime = (f(a+delta_a)-f(a))/delta_a \n",
    "    tan = f(a)+fprime*(x_dev-a)\n",
    "    # plot of the function and the tangent\n",
    "    plt.plot(x_dev, tan, color = \"black\", linestyle=\"dashed\")\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=[.5, 1],\n",
    "           ncol=2, fancybox=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules for Derivatives + Lab (in Appendix)\n",
    "- Power Rule\n",
    "    - $f(x) = x^r $ ---> $ f'(x) = r*x^{r-1} $\n",
    "- Constant Factor Rule\n",
    "    - $\\frac{\\Delta f}{\\Delta x}(a*f(x)) = a * \\frac{\\Delta f}{\\Delta x}*f(x) $  \n",
    "- Addition Rule\n",
    "    - the derivative of multiple terms (being added) is the same as each term being derived\n",
    "- Chain Rule\n",
    "    - $ F(x) = f(g(x)) $ ---> $ F'(x) = f'(g(x))*g'(x) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_term_derivative(term):\n",
    "    constant = term[0]*term[1]\n",
    "    exponent = term[1] - 1 \n",
    "    return np.array([constant, exponent])\n",
    "# return something which looks like: np.array([constant, exponent])\n",
    "def find_derivative(function_terms):\n",
    "    der_array = np.zeros(np.shape(function_terms))\n",
    "    for i in range(int(np.shape(function_terms)[0])):\n",
    "        der_array[i] = find_term_derivative(function_terms[i])\n",
    "    return der_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives: Conclusion\n",
    "- minima/maxima exist where $ f'(x) = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Gradient Descent\n",
    "- minimize cost function\n",
    "    - in linear regression, the cost function is RSS (Residual Sum of Squared Errors) and is reduced by adjusting the parameters (slopes and intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent: Step Sizes (learning rate)\n",
    "- slope of the cost curve tells us our step size\n",
    "\n",
    "We use the following procedure to find the ideal $m$: \n",
    "1.  Randomly choose a value of $m$ (random initialization)\n",
    "2.  Update $m$ with the formula $ m = (-a) * slope_{m = i} + m_i, a = learning rate$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent in 3D and The Gradient in Gradient Descent\n",
    "- gradient descent: taking the shortest path to descend towards our minimum\n",
    "- we denote the gradient of a function, $f(x, y)$, with $\\nabla f(x, y)$\n",
    "- if $\\frac{df}{dy} > \\frac{df}{dx}$, we should make that move more in the $y$ direction than the $x$ direction, and vice versa. and you should move $ \\frac{\\delta f}{\\delta y} $ divided by $ \\frac{\\delta f}{\\delta x} $. So for example, when $ \\frac{\\delta f}{\\delta y}f(x, y) = 3 $ , and $ \\frac{\\delta f}{\\delta x}f(x, y) = 2$, you traveled in line with a slope of 3/2.\n",
    "- gradient ascent = $\\nabla f(x, y)$, gradient descent = $-\\nabla f(x, y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient to Cost Function + Lab (in appendix)\n",
    "- cost function for linear regression:\n",
    "$$\n",
    "\\begin{align}\n",
    "J(m, b) & = \\sum_{i=1}^{n}(y_i - \\hat{y})^2 &&\\text{cost function, $J$, (representing RSS)}\\\\\n",
    "J(m, b) & = \\sum_{i=1}^{n}(y_i - (mx_i + b))^2 &&\\text{notice $\\hat{y} = mx + b$}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Take Partial Derivatives of the Cost Function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta J}{\\delta m}J(m, b) & = \\boldsymbol{\\frac{\\delta J}{\\delta m}}(y - (mx + b))^2 = -2x*(y - (mx + b )) &&\\text{partial derivative with respect to} \\textbf{ m}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta J}{\\delta b}J(m, b) & = \\boldsymbol{\\frac{\\delta J}{\\delta m}}(y - (mx + b))^2 = -2*(y - (mx + b)) &&\\text{partial derivative with respect to} \\textbf{ b}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Replace $y - \\hat{y}$ with $\\epsilon$, our error:\n",
    "\n",
    "$$ \\frac{dJ}{dm}J(m,b) = -2*x(y - (mx + b )) = -2*x(y - \\hat{y})  = -2x*\\epsilon $$\n",
    "\n",
    "$$ \\frac{dJ}{db}J(m,b) = -2*(y - (mx + b)) -2*(y - \\hat{y}) = -2\\epsilon $$\n",
    "\n",
    "$$ \\frac{dJ}{dm}J(m,b) = -2*\\sum_{i=1}^n x(y_i - \\hat{y}_i)  = -2*\\sum_{i=1}^n x_i*\\epsilon_i$$\n",
    "$$ \\frac{dJ}{db}J(m,b) = -2*\\sum_{i=1}^n(y_i - \\hat{y}_i) = -2*\\sum_{i=1}^n \\epsilon_i$$\n",
    "\n",
    "In the context of gradient descent, we use these partial derivatives to take a step size.  Remember that our step should be in the opposite direction of our partial derivatives as we are *descending* towards the minimum.  So to take a step towards gradient descent we use the general formula of:\n",
    "\n",
    "`current_m` =  `old_m` $ - \\frac{dJ}{dm}J(m,b)$\n",
    "\n",
    "`current_b` =  `old_b` $ - \\frac{dJ}{db}J(m,b) $\n",
    "\n",
    "or in the code that we just calculated:\n",
    "\n",
    "`current_m` = `old_m` $ -  (-2*\\sum_{i=1}^n x_i*\\epsilon_i )$\n",
    "\n",
    "`current_b` =  `old_b` $ - ( -2*\\sum_{i=1}^n \\epsilon_i )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Gradient Descent Lab\n",
    "Note that, for our gradients, when having multiple predictors $x_j$ with $j \\in 1,\\ldots, k$\n",
    "\n",
    "$$ \\frac{dJ}{dm_j}J(m_j,b) = -2\\sum_{i = 1}^n x_{j,i}(y_i - (\\sum_{j=1}^km{x_{j,i}} + b)) = -2\\sum_{i = 1}^n x_{j,i}*\\epsilon_i$$\n",
    "$$ \\frac{dJ}{db}J(m_j,b) = -2\\sum_{i = 1}^n(y_i - (\\sum_{j=1}^km{x_{j,i}} + b)) = -2\\sum_{i = 1}^n \\epsilon_i $$\n",
    "    \n",
    "\n",
    "So we'll have one gradient per predictor along with the gradient for the intercept!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>\n",
    "\n",
    "\n",
    "## Topic 24: Feature Selection, Ridge and Lasso (Regularization)\n",
    "- Regularization: general technique of battling overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge and Lasso Regression (L2/L1 Norm Regularization) + Lab\n",
    "[Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) and [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html).\n",
    "- Lasso:\"Least Absolute Shrinkage and Selection Operator\"\n",
    "    - performs estimation and selection simultaneously\n",
    "    - better when we have more features\n",
    "- add penalty for large coefficients (AKA penalized estimations)\n",
    "- Advantages:\n",
    "    - reduce model complexity\n",
    "    - may prevent from overfitting\n",
    "    - Some of them may perform variable selection at the same time (when coefficients are set to 0)\n",
    "    - can be used to counter multicollinearity\n",
    "- must standardize data before using either of these\n",
    "- scale after train-test-split to prevent data-leakage\n",
    "- only fit scaler on training data\n",
    "\n",
    "$ \\text{cost_function_linear}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$\n",
    "    \n",
    "$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$\n",
    "\n",
    "$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$\n",
    "\n",
    "- [Full Code](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/) of Ridge and Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T00:25:35.411906Z",
     "start_time": "2021-01-27T00:25:28.671866Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = data[['mpg']] # t-t split X and y\n",
    "X = data.drop(['mpg', 'car name', 'origin'], axis=1)\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "scale = MinMaxScaler() # scale after t-t-split\n",
    "X_train_transformed = scale.fit_transform(X_train)\n",
    "X_test_transformed = scale.transform(X_test)\n",
    "ridge = Ridge(alpha=0.5) # fit our models, alpha is our lambda\n",
    "ridge.fit(X_train_transformed, y_train) \n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X_train_transformed, y_train)\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train_transformed, y_train)\n",
    "y_h_ridge_train = ridge.predict(X_train_transformed) #generate predictions\n",
    "y_h_ridge_test = ridge.predict(X_test_transformed)\n",
    "y_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\n",
    "y_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n",
    "y_h_lin_train = lin.predict(X_train_transformed)\n",
    "y_h_lin_test = lin.predict(X_test_transformed)\n",
    "print('Train Error Ridge Model', np.sum((y_train - y_h_ridge_train)**2))\n",
    "print('Test Error Ridge Model', np.sum((y_test - y_h_ridge_test)**2))\n",
    "print('Train Error Lasso Model', np.sum((y_train - y_h_lasso_train)**2))\n",
    "print('Test Error Lasso Model', np.sum((y_test - y_h_lasso_test)**2))\n",
    "print('Train Error Unpenalized Linear Model (RSS', np.sum((y_train - lin.predict(X_train_transformed))**2))\n",
    "print('Test Error Unpenalized Linear Model (RSS', np.sum((y_test - lin.predict(X_test_transformed))**2))\n",
    "print('Ridge parameter coefficients:', ridge.coef_)\n",
    "print('Lasso parameter coefficients:', lasso.coef_)\n",
    "print('Linear model parameter coefficients:', lin.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T00:51:26.249581Z",
     "start_time": "2021-01-27T00:51:26.163448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lab code\n",
    "# Remove \"object\"-type features from X\n",
    "cont_features = [col for col in X.columns if X[col].dtype in [np.float64, np.int64]]\n",
    "# Remove \"object\"-type features from X_train and X_test\n",
    "X_train_cont = X_train.loc[:, cont_features]\n",
    "X_test_cont = X_test.loc[:, cont_features]\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.impute import SimpleImputer\n",
    "# SimpleImputer fills the missing values in data using {Strategy} of the columns ex. {mean/median/most_frequent/constant}\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Build naive linreg model. Impute missing values with median using SimpleImputer\n",
    "impute = SimpleImputer(strategy='median')\n",
    "X_train_imputed = impute.fit_transform(X_train_cont)\n",
    "X_test_imputed = impute.transform(X_test_cont)\n",
    "# Fit the mode\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_imputed, y_train)\n",
    "# Print R2 and MSE for training and test sets\n",
    "print('Training r^2:', linreg.score(X_train_imputed, y_train)) # r^2\n",
    "print('Test r^2:', linreg.score(X_test_imputed, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, linreg.predict(X_train_imputed)))\n",
    "print('Test MSE:', mean_squared_error(y_test, linreg.predict(X_test_imputed)))\n",
    "\n",
    "# Scale the train and test data (Normalize), errors will remain the same\n",
    "ss = StandardScaler()\n",
    "X_train_imputed_scaled = ss.fit_transform(X_train_imputed)\n",
    "X_test_imputed_scaled = ss.transform(X_test_imputed)\n",
    "# Fit the model \n",
    "linreg_norm = LinearRegression()\n",
    "linreg_norm.fit(X_train_imputed_scaled, y_train)\n",
    "# Print R2 and MSE for training and test sets\n",
    "print('Training r^2:', linreg_norm.score(X_train_imputed_scaled, y_train))\n",
    "print('Test r^2:', linreg_norm.score(X_test_imputed_scaled, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, linreg_norm.predict(X_train_imputed_scaled)))\n",
    "print('Test MSE:', mean_squared_error(y_test, linreg_norm.predict(X_test_imputed_scaled)))\n",
    "\n",
    "# CInclude categorical variables\n",
    "features_cat = [col for col in X.columns if X[col].dtype in [np.object]]\n",
    "X_train_cat = X_train.loc[:, features_cat]\n",
    "X_test_cat = X_test.loc[:, features_cat]\n",
    "# Fill missing values with the string 'missing'\n",
    "X_train_cat.fillna(value='missing', inplace=True)\n",
    "X_test_cat.fillna(value='missing', inplace=True)\n",
    "# OneHotEncode categorical variables\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "# Transform training and test sets\n",
    "X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "X_test_ohe = ohe.transform(X_test_cat)\n",
    "# Convert these columns into a DataFrame and add to continuous DF\n",
    "columns = ohe.get_feature_names(input_features=X_train_cat.columns)\n",
    "cat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\n",
    "cat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n",
    "X_train_all = pd.concat([pd.DataFrame(X_train_imputed_scaled), cat_train_df], axis=1)\n",
    "X_test_all = pd.concat([pd.DataFrame(X_test_imputed_scaled), cat_test_df], axis=1)\n",
    "# sever overfitting may show if r^\n",
    "\n",
    "# Lasso \n",
    "lasso = Lasso() # alpha/lambda default = 1, can iterate through different lambda's to lower test mse\n",
    "lasso.fit(X_train_all, y_train)\n",
    "print('Training r^2:', lasso.score(X_train_all, y_train))\n",
    "print('Test r^2:', lasso.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, lasso.predict(X_train_all)))\n",
    "print('Test MSE:', mean_squared_error(y_test, lasso.predict(X_test_all)))\n",
    "\n",
    "# Ridge\n",
    "ridge = Ridge() # alpha/lambda default = 1, can iterate through different lambda's to lower test mse\n",
    "ridge.fit(X_train_all, y_train)\n",
    "print('Training r^2:', ridge.score(X_train_all, y_train))\n",
    "print('Test r^2:', ridge.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, ridge.predict(X_train_all)))\n",
    "print('Test MSE:', mean_squared_error(y_test, ridge.predict(X_test_all)))\n",
    "\n",
    "print(sum(abs(ridge.coef_) < 10**(-10))) # Number of Ridge params almost zero\n",
    "print(sum(abs(lasso.coef_) < 10**(-10))) # Number of Lasso params almost zero\n",
    "print(len(lasso.coef_)) # number of variables unselected (coeff = 0)\n",
    "print(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_)) # % / 100 of these variables to all variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, y):\n",
    "    '''Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n",
    "    train and test DataFrames with targets'''\n",
    "    # Train-test split (75-25), set seed to 10\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)\n",
    "    # Remove \"object\"-type features and SalesPrice from X\n",
    "    cont_features = [col for col in X.columns if X[col].dtype in [np.float64, np.int64]]\n",
    "    X_train_cont = X_train.loc[:, cont_features]\n",
    "    X_test_cont = X_test.loc[:, cont_features]\n",
    "    # Impute missing values with median using SimpleImputer\n",
    "    impute = SimpleImputer(strategy='median')\n",
    "    X_train_imputed = impute.fit_transform(X_train_cont)\n",
    "    X_test_imputed = impute.transform(X_test_cont)\n",
    "    # Scale the train and test data\n",
    "    ss = StandardScaler()\n",
    "    X_train_imputed_scaled = ss.fit_transform(X_train_imputed)\n",
    "    X_test_imputed_scaled = ss.transform(X_test_imputed)\n",
    "    # Create X_cat which contains only the categorical variables\n",
    "    features_cat = [col for col in X.columns if X[col].dtype in [np.object]]\n",
    "    X_train_cat = X_train.loc[:, features_cat]\n",
    "    X_test_cat = X_test.loc[:, features_cat]\n",
    "    # Fill nans with a value indicating that that it is missing\n",
    "    X_train_cat.fillna(value='missing', inplace=True)\n",
    "    X_test_cat.fillna(value='missing', inplace=True)\n",
    "    # OneHotEncode Categorical variables\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "    X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "    X_test_ohe = ohe.transform(X_test_cat\n",
    "    columns = ohe.get_feature_names(input_features=X_train_cat.columns)\n",
    "    cat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\n",
    "    cat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n",
    "    # Combine categorical and continuous features into the final dataframe\n",
    "    X_train_all = pd.concat([pd.DataFrame(X_train_imputed_scaled), cat_train_df], axis=1)\n",
    "    X_test_all = pd.concat([pd.DataFrame(X_test_imputed_scaled), cat_test_df], axis=1)\n",
    "    return X_train_all, X_test_all, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph the training and test error to find optimal alpha values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "alphas = []\n",
    "for alpha in np.linspace(0, 200, num=50):\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_train_all, y_train)\n",
    "    train_preds = lasso.predict(X_train_all)\n",
    "    train_mse.append(mean_squared_error(y_train, train_preds))\n",
    "    test_preds = lasso.predict(X_test_all)\n",
    "    test_mse.append(mean_squared_error(y_test, test_preds))\n",
    "    alphas.append(alpha)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alphas, train_mse, label='Train')\n",
    "ax.plot(alphas, test_mse, label='Test')\n",
    "ax.set_xlabel('Alpha')\n",
    "ax.set_ylabel('MSE')\n",
    "# np.argmin() returns the index of the minimum value in a list\n",
    "optimal_alpha = alphas[np.argmin(test_mse)]\n",
    "# Add a vertical line where the test MSE is minimized\n",
    "ax.axvline(optimal_alpha, color='black', linestyle='--')\n",
    "ax.legend();\n",
    "print(f'Optimal Alpha Value: {int(optimal_alpha)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature and Model Selection: AIC and BIC\n",
    "- AIC and BIC:\n",
    "    - give comprehensive measure of model performance taking to account additional variables\n",
    "- AIC (Akaike's Information Criterion):\n",
    "    - generally used to compare each candidate model\n",
    "    - for every model that uses MLE (Maximum Likelihood Estimation), the log-likelihood is automatically computed, so the AIC is very easy to calculate\n",
    "    - acts as penalized log-likelihood criterion, giving a balance between a good fit (high value of log-likelihood) and complexity (complex models are penalized more than fairly simple ones)\n",
    "    - unbounded but lowest AIC should be selected\n",
    "    - built into statsmodels and in sklearn (such as LassoLarsIC)\n",
    "    - $ \\text{AIC} = -2\\ln(\\hat{L}) + 2k $\n",
    "    \n",
    "        Where:\n",
    "\n",
    "        - $k$ : length of the parameter space (i.e. the number of features)\n",
    "        - $\\hat{L}$ : the maximum value of the likelihood function for the model\n",
    "        \n",
    "    - Another way to phrase the equation is:\n",
    "\n",
    "        $ \\text{AIC(model)} = - 2 * \\text{log-likelihood(model)} + 2 * \\text{length of the parameter space} $\n",
    "\n",
    "\n",
    "- BIC (Bayesian Information Criterion):\n",
    "    - penalty is slightly changed and depends on the number of rows in the dataset\n",
    "    - Like the AIC, the lower your BIC, the better your model is performing\n",
    "    - $ \\text{BIC} = - 2\\ln(\\hat{L}) + \\ln(n) * k $\n",
    "\n",
    "        where:\n",
    "\n",
    "        - $\\hat{L}$ and $k$ are the same as in AIC\n",
    "        - $n$ : the number of data points (the sample size)\n",
    "        \n",
    "    - Another way to phrase the equation is:\n",
    "\n",
    "    $ \\text{BIC(model)} = -2 * \\text{log-likelihood(model)} + \\text{log(number of observations)} * \\text{(length of the parameter space)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Methods\n",
    "- Feature selection benefits include:\n",
    "    - Decrease in computational complexity: With reduced features, it is easier to compute the model parameters and the amount of data storage required to maintain the features of your model decreases\n",
    "    - Understanding your data: With feature selection, you gain more understanding of how features relate to one another\n",
    "- Types of feature selection\n",
    "    - There are different strategie/methods you can use to process features in an efficient way: \n",
    "        * Domain knowledge: \n",
    "            - knowledge/thoughts/research on important features\n",
    "        * Wrapper methods: \n",
    "            - determine optimal subset of features using different combinations of features to train models and then calculating performance\n",
    "            - Every subset is used, so this can end up being very computationally intensive and time consuming\n",
    "            - highly effective, but challenging with large feature sets\n",
    "            - ex. Recursive Feature Elimination (RFE) in linear regression\n",
    "                - opposite of RFE is Forward Selection\n",
    "        * Filter methods\n",
    "            - carried out as a pre-processing step\n",
    "            - different metrics are used to determine feature reduction\n",
    "            - remove variables considered redundant\n",
    "            - data scientist determines the cut-off point (to keep top n features)\n",
    "            - ex. in linear regression, eliminate features that are highly correlated with one another\n",
    "            - ex. use variance threshold\n",
    "        * Embedded methods\n",
    "            - included in actual formulation of ML algorithm\n",
    "            - common ex. regularization, like Lasso\n",
    "- full code shows data preprocessing, running a baseling model, adding polynomial features, filter methods, wrapper methods, Embedded Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_poly_train = pd.DataFrame(poly.fit_transform(X_train_transformed), columns=poly.get_feature_names(features.columns))\n",
    "X_poly_test = pd.DataFrame(poly.transform(X_test_transformed), columns=poly.get_feature_names(features.columns))\n",
    "# Fliter Methods\n",
    "from sklearn.feature_selection import VarianceThreshold, f_regression, mutual_info_regression, SelectKBest\n",
    "threshold_ranges = np.linspace(0, 2, num=6)\n",
    "for thresh in threshold_ranges:\n",
    "    print(thresh)\n",
    "    selector = VarianceThreshold(thresh)\n",
    "    reduced_feature_train = selector.fit_transform(X_poly_train)\n",
    "    reduced_feature_test = selector.transform(X_poly_test)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(reduced_feature_train, y_train)\n",
    "    run_model(lr, reduced_feature_train, reduced_feature_test, y_train, y_test)\n",
    "    print('--------------------------------------------------------------------')\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression)\n",
    "X_k_best_train = selector.fit_transform(X_poly_train, y_train)\n",
    "X_k_best_test= selector.transform(X_poly_test)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_k_best_train ,y_train)\n",
    "run_model(lr,X_k_best_train,X_k_best_test,y_train,y_test)\n",
    "\n",
    "selector = SelectKBest(score_func=mutual_info_regression)\n",
    "X_k_best_train = selector.fit_transform(X_poly_train, y_train)\n",
    "X_k_best_test= selector.transform(X_poly_test)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_k_best_train ,y_train)\n",
    "run_model(lr,X_k_best_train,X_k_best_test,y_train,y_test)\n",
    "# Wrapper Methods\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "rfe = RFECV(LinearRegression(),cv=5)\n",
    "X_rfe_train = rfe.fit_transform(X_poly_train, y_train)\n",
    "X_rfe_test = rfe.transform(X_poly_test)\n",
    "lm = LinearRegression().fit(X_rfe_train, y_train)\n",
    "run_model(lm, X_rfe_train, X_rfe_test, y_train, y_test)\n",
    "print ('The optimal number of features is: ', rfe.n_features_)\n",
    "# Embedded Methods\n",
    "from sklearn.linear_model import LassoCV\n",
    "lasso = LassoCV(max_iter=100000, cv=5)\n",
    "lasso.fit(X_train_transformed, y_train)\n",
    "run_model(lasso,X_train_transformed, X_test_transformed, y_train, y_test)\n",
    "print('The optimal alpha for the Lasso Regression is: ', lasso.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions to Linear Models Lab\n",
    "- fulll code: Look at baseline model, include interactions, include polynomials, full model R^2, find best Lasso regularization parameter, analyzing the final result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data + Lab\n",
    "- generate data to evaluate and compare ML algorithms\n",
    "- why generated datasets are preferred over real-world datasets:\n",
    "    - Quick and easy generation - save data collection time and efforts\n",
    "    - Predictable outcomes - have a higher degree of confidence in the result\n",
    "    - Randomization - datasets can be randomized repeatedly to inspect performance in multiple cases\n",
    "    - Simple data types - easier to visualize data and outcomes\n",
    "- sklearn.datasets.make_blobs\n",
    "    - generate any number of classes\n",
    "    - can be used with a number of classifiers to see how accurately they perform\n",
    "- sklearn.datasets.make_moons\n",
    "    - used for binary classification problems and generates moon shaped patterns\n",
    "    - allows you to specify the level of noise in the data\n",
    "    - can try non-linear classification functions (like sigmoid and tanh etc.)\n",
    "- sklearn.datasets.make_circles\n",
    "    - creates values in the form of concentric circles\n",
    "    - also suitable for testing complex, non-linear classifiers\n",
    "- sklearn.datasets.make_regression\n",
    "    - used to test regression algorithms\n",
    "    - can further tweak the generated parameters to create non-linear relationships that can be solved using non-linear regression techniques (ex. squaring or cubing y)\n",
    "- [sklearn API reference](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets) where the datasets module is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=100, centers=3, n_features=2, cluster_std=2)\n",
    "df = pd.DataFrame(dict(x=X[:, 0], y=X[:, 1], label=y))\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(n_samples=100, noise=0.05)\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>\n",
    "\n",
    "\n",
    "## Extra Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.randrange(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "time_spent = timeit.default_timer() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "np.sum(A)\n",
    "la.norm(a)\n",
    "la.eig(A) # [0] for eigenvalues, [1] for eigenvectors\n",
    "np.zeros/ones(r, c)\n",
    "np.eye(4) # or\n",
    "np.identity(4, dtype=int)\n",
    "la.inv(A)\n",
    "np.power(A,B) # elements in A raised to the power of elements in B\n",
    "np.matrix.round(A)\n",
    "la.solve(A,s)\n",
    "np.square/sqrt(x)\n",
    "np.arccos/sin/tan(x) # in radians\n",
    "np.random.rand(r,c) # 0-1 uniform distribution\n",
    "y_randterm = np.random.normal(loc/mu,scale/std,size) # draws from normal distribution with mu = loc and std = scale\n",
    "np.random.choice(a, size=int/tuple, replace=False)\n",
    "np.random.randint(low,high,size,dtype)\n",
    "random_state = np.random.RandomState(42)\n",
    "a = np.arange(start, stop, step, dtype)\n",
    "np.asmatrix(A) # or np.mat(A), returns matrix([[,],[,]]) instead of array\n",
    "np.mat(A)\n",
    "# .shape or np.shape(A)\n",
    "# .size\n",
    "# X.T or np.transpose(X)\n",
    "x = np.linspace(start, stop, num)\n",
    "np.argmin/max() # Returns the indices of the min/max values along an axis.\n",
    "import sys\n",
    "np.savetxt(sys.stdout, bval_RSS, '%16.2f') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Create Empty lists for storing X and y values \n",
    "data = []\n",
    "# Read the data from the csv file\n",
    "with open('windsor_housing.csv') as f:\n",
    "    raw = csv.reader(f) # <_csv.reader object at 0x7f22780cc7b0>\n",
    "    # Drop the very first line as it contains names for columns - not actual data \n",
    "    print(next(raw)) # ['lotsize', 'bedrooms', 'bathrms', 'stories', 'driveway', 'recroom', 'fullbase', 'gashw', 'airco', 'garagepl', 'prefarea', 'price']\n",
    "            \n",
    "    # Read one row at a time. Append one to each row\n",
    "    for row in raw:\n",
    "        ones = [1.0]\n",
    "        for r in row:\n",
    "            ones.append(float(r)) # 'str' to 'float', append the row to [1.0]\n",
    "        # Append the row to data \n",
    "        data.append(ones)\n",
    "data = np.array(data)\n",
    "data[:5,:] #first five rows of all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter([1,2,3])\n",
    "\n",
    "print(iterator) # <list_iterator object at 0x7f2278116ac0>\n",
    "print(next(iterator)) # 1\n",
    "print(next(iterator)) # 2\n",
    "\n",
    "help(class_)\n",
    "\n",
    "type(var)\n",
    "\n",
    "isinstance(instance, class_/superclass)\n",
    "isinstance(class_, superclass)\n",
    "\n",
    "repr(obj), str(obj), len(obj) # directly calls special methods, same as obj.__repr/str__()\n",
    "\n",
    "min(var), max(var)\n",
    "\n",
    "vars(obj) # obj with __dict__ attr, returns this attr\n",
    "\n",
    "sorted(list_, key=lambda x: x.var, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized RMSE\n",
    "root_mean_sq_err/(y_train.max() - y_train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T02:43:11.982228Z",
     "start_time": "2021-01-22T02:43:11.978863Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "datetime.date(2015, 7, 10).weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Methods\n",
    "https://docs.python.org/3/reference/datamodel.html#special-method-names\n",
    "- repr: unambiguous representation to the object and used for debugging, logging, etc. Meant to be seen by other developers. calling an str special method if one is not defined will use repr as fallback.\n",
    "- str: readable representation of the object. Meant to be used as a display to end user.\n",
    "- good rule for the above methods is to display something that can allow you to recreate that same object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __repr__(self):\n",
    "    return \"Employee('{}', '{}', '{}')\".format(self.first, self.last, self.pay)\n",
    "\n",
    "def __str__(self):\n",
    "    return '{} - {}'.format(self.fullname(), self.email)\n",
    "\n",
    "def __add__(self, other):        # ex. int.__add__(1,2) same as 1+2, str.__add__('a','b') same as 'a' +'b'\n",
    "    return self.pay + other.pay\n",
    "    return NotImplemeneted # way to fall back on other object to handle operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dictionary\n",
    "dict(sorted(genre_count_dict.items(), key=lambda item: item[1], reverse=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dataframes\n",
    "new_df = leftdf.join(right, on=‘col’, how=‘left’) # may need rightdf.set_index(‘col’, inplace=True)\n",
    "new_df = pd.merge(leaftdf, rightdf, left_on, right_on, how)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "227.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
