{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # [StatQuest ML playlist](https://youtube.com/playlist?list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF)\n",
    "- Evaluation Metrics on other notes.\n",
    "- Bias: inability for ML method to capture true relationship\n",
    "- Variance: difference in fits between datasets\n",
    "\n",
    "### ROC/AUC\n",
    "- Calculation threshold: in Log Reg the center would be .5\n",
    "- Threshold with more FP’s is better when we want to find more/all positives\n",
    "- Threshold with more FN’s is better when we  we want to find more/all negatives\n",
    "- ROC curve shows (y vs x) TP rate (sensitivity/recall) vs FP rate (1-specificity) and summarizes confusion matrices that each threshold produces\n",
    "- TP rate = recall\n",
    "- FP rate = FP/(FP+TN)\n",
    "- Diagonal (y = x) shows where TP rate = FP rate\n",
    "- optimal threshold are on left corners depending on how many FPs we’re willing to accept\n",
    "- AUC is used to compare ROC curves, larger AUC is better\n",
    "- Alternative to ROC: replace FP rate with Precision, not affected by class imbalance, so often more useful if we have more positives in the data\n",
    "\n",
    "\n",
    "### Odds/Log(Odds):\n",
    "- odds are not probabilities, but ratios of something happening to something not happening\n",
    "- probabilities are the ratios of something happening to everything happening\n",
    "- odds = P(A) / P’(A)\n",
    "    - 0 < odds < 1 if odds against A\n",
    "    - 1 < odds < inf if odds for A\n",
    "- Log(odds) creates symmetry:\n",
    "    - Negative if odds against A\n",
    "    - Positive if odds for A\n",
    "    - Can be calculated with probabilities: logit function = log(p/(1-p))\n",
    "    - Histogram of log odds is normally distributed which is useful for solving certain stats problems\n",
    "\n",
    "### Odds Ratio/ Log(Odds Ratio)\n",
    "- odds ratio = oddsA / oddsB\n",
    "    - 0 < odds < 1 if denominator> numerator\n",
    "    - 1 < odds < inf if numerator > denominator\n",
    "- Log(odds ratio) creates symmetry:\n",
    "    - Negative if oddsB > oddsA\n",
    "    - Positive if oddsA > oddsB\n",
    "- can see these ratios in confusion matrices with true positive rate and false positive rate\n",
    "- Like R-squared because these ratios indicate a relationship between two things (ex. Actual values vs predicted values) and these ratios correspond to effect size. A larger odds ratio (or log(odds ratio) means that predicted values are a good predictor of actual values and a smaller odds ratio means the predicted values are not a good predictor of the actual values\n",
    "- 3 ways to determine if odds ratio (or log(odds ratio) is statistically significant: \n",
    "    - Fischers Exact Test\n",
    "    - Chi-Squared Test (for p-value)\n",
    "    - The Wald Test (for confidence interval and p-value)\n",
    "        - Wald test see how many std’s the log(odds) is from zero\n",
    "        - std = sqrt(sum(1/TP+1/TN+1/FP+1/FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "- fits logistic function instead of a line\n",
    "- for function line, y is probability of class given X\n",
    "- class predicted by X with classification threshold 0 < p < 1\n",
    "- Unlike normal regression, we can’t easily compare the complex model (multiple features) to the simple model\n",
    "- Use Wald’s test to test if the variable’s effect on the prediction is significantly different than 0. If not, it means the variable is mot helping the prediction \n",
    "- MLE is used to pick the best logistic regression line\n",
    "\n",
    "### Likelihood vs Probability:\n",
    "- Probability: P(data | distribution), area under distribution curve\n",
    "- Likelihood: P(distribution | data) ex. P(mean =34g and std=2.5g | 34g object)\n",
    "\n",
    "### Log Reg Details Coeffs:\n",
    "- Generalized Linear Models (GLM): Log Reg and Linear Models\n",
    "- y-acis is transformed from a 0 to 1 range to a -inf to inf range with the logit function. y-axis goes from P(A) scale to a log(odds of A) scale\n",
    "- coefficients represent this new line:\n",
    "- Intercept: when x = 0, the log(odds of A) is ‘coeff’. If ‘coeff’ is negative, the odds are against A, if it’s positive, the odds are in favor of A\n",
    "- Wald’s Test: z-value is the estimated intercept/slope divided by the standard error (SE for the estimated intercept/slope), or the number of std’s the estimated intercept/slope is away from away from 0 on a standard normal curve. If |z-value| is less than 2, it is not statistically significant from 0, and should be confirmed by a larger p-value.\n",
    "- variable coeffs: for a one unit increase in x, the log(oddsA) increases by ‘coeff’\n",
    "- Log reg on categorical variables is similar to how we do a t-test usine linear models\n",
    "- “Intercept” the log(oddsA) for categorical vaiable not true/existing\n",
    "- “Cat coeff” term is the log(odds ratio) that tells you how much the category being true increases or decreases log(oddsA)\n",
    "- Coefficients are the same as in a linear model except that they are on a log(odds) scale\n",
    "\n",
    "### Log Reg Details MLE:\n",
    "- 1. Project each point onto candidate line (line that is transformed with logit function), giving each point a log(oddsA) value  (log(oddsA) is on y axis\n",
    "- 2. These log(oddsA) values are transformed to probability values using e^log(oddsA) / 1 + e^log(oddsA) (<- like inverse of logit function)\n",
    "- 3. Calculate the likelihood of A given the logistic regression line. Also remember the probability for A is also the y-value of this log reg line, not the area under the curve\n",
    "- 4. The likelihood for all A is the product of the individual A likelihoods\n",
    "- 5. The likelihood for A’ is 1 minus the y-value (on the log reg line) and, again, the likelihood for all A’ is the product of the individual A’ likelihoods\n",
    "- 6. The likelihood for all A and all A’ is the product of both. Statisticians like to calculate the log of the likelihood (log() each of the individual likelihoods), but either way, the same log reg lin3 maximizes both of these.\n",
    "\n",
    "### Log Reg Details R_squared and p-values:\n",
    "- McFadden’s pseudo R^2 = ( LL(overallprobability) - LogLikelihood(fittedline) ) / ( LL(overallprobabilty) - LL(saturated model) )\n",
    "- p-value = chi-squared value = 2 ( ( LL(saturated model) - LL(fit) ) - ( LL(saturated model) - LL(overall probability) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Multinomial) Naive Bayes:\n",
    "- P(event | class), ex. P(word occuring| spam class)\n",
    "- Initial guess: prior probability P(not spam)\n",
    "- guesses estimated from classifications in the training data\n",
    "- Score observed event gets if it is prior class = prior * sum( P(each observed events | prior class) )\n",
    "- Scores proportional to P( prior class | observed events )\n",
    "- ex. P(Not Spam) * P(Lunch|Not Spam) * P(Money|Not Spam)^4\n",
    "- alpha = number of counts added to each individual event (e.g. words)\n",
    "- naive : all individual event orders don’t matter, which means there is high bias and low variance\n",
    "\n",
    "### Gaussian Naive Bayes:\n",
    "- 2 normal distributions set for each feature (one for each class) with their mean and std\n",
    "- Score observed event gets if it is prior class = prior * sum( L(each observed events | prior class) )\n",
    "- Likelihood is y-axis coordinate on normal curve\n",
    "- like before, initial guess is made with training data (most common class)\n",
    "- Prevent underflow (very small likelihood) by ln() the whole score (or ln() of the P(class) and all the likelihoods)\n",
    "- Pick great score / log scores\n",
    "- May only need one or a few features to make classifications, find these with CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees:\n",
    "\n",
    "- Asks T/F question to classify. Classifications can be categories or numeric\n",
    "- Can be based on ranked data\n",
    "- Root (node): top of tree, feature with lowest Gini impurity\n",
    "- Middle nodes: set if Gini impurity is less than if we classify and don’t separate the node\n",
    "- Leaves (nodes): classifications, set if Gini impurity is less than if we separate and don’t separate the node\n",
    "    - impure: when leaf isnt 100% one classification\n",
    "- Gini measures impurity: 1-P(yes)^2-P(no)^2\n",
    "- Total Gini impurity: weighted averages of leaf node impurities = gini1 * (leaf1/total) + gini2 * (leaf2/total)\n",
    "- With continuous data: sort the feature data and find lowest impurity for separating the data by the average of any two adjacent values\n",
    "- With ranked data: similar to numeric data but instead, find the impurity scores for all the possible ranks (<= rank)\n",
    "- With multiple choice data: calculate an impurity score for each choice, as well as each possible combination\n",
    "- **Part 2 - feature selection and missing data**\n",
    "- Create a threshold such that the feature reduction has to be large enough to include the feature in the tree\n",
    "    - this simplifies model and reduces overfitting\n",
    "- For binary categorical variable missing data, we could replace with the mode, or impute with value from the most correlated feature column\n",
    "- For continuous variable missing data, we couple replace with the mean/median or impute with the predicted value from the linear regression line with the feature produces with the highest correlated feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Trees\n",
    "\n",
    "- Each leaf represents a numeric value\n",
    "- Split the data into two groups by finding the threshold that minimizes the SSR. Plot SSR vs feature threshold\n",
    "- Root node is feature threshold with minimum SSR\n",
    "- For multiple features, compare minimum SSRs between the features\n",
    "- Pruning: Cost Complexity Pruning AKA Weakest Link Pruning\n",
    "- 1. Calculate SSR for each pruned tree (not pruned to fully pruned)\n",
    "- 2. Find tree score by adding Tree complexity penalty (aT) where T is the number of Terminal/leaf nodes in the tree/subtree, alpha is tuning parameter we find using CV. Bigger alpha makes for bigger complexity penalty \n",
    "- 3. Pick lowest tree score\n",
    "- \n",
    "- 1. Using all the data, build a full size regression tree and \n",
    "\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "- 1.  Create a “bootstrapped” dataset\n",
    "- 2. Create a decision tree but only use a random subset of features at each step\n",
    "- 3. From the random subset, make a feature the root node\n",
    "- 4.  Eliminate the feature, randomly select the remaining features, and make a middle node. Repeat until we are done with the features\n",
    "- 5. Repeat 1-4\n",
    "- Bagging: bootstrapping the data and then using the aggregate to classify\n",
    "- Out of Bag (Boot) dataset\n",
    "- Measure accuracy by proportion of out of bag samples that were correctly labelled classified (1 - out of bag error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cxzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
