{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## StatsQuest ML playlist\n",
    "- Evaluation Metrics on other notes.\n",
    "- Bias: inability for ML method to capture true relationship\n",
    "- Variance: difference in fits between datasets\n",
    "\n",
    "### ROC/AUC\n",
    "- Calculation threshold: in Log Reg the center would be .5\n",
    "- Threshold with more FP’s is better when we want to find more/all positives\n",
    "- Threshold with more FN’s is better when we  we want to find more/all negatives\n",
    "- ROC curve shows (y vs x) TP rate (sensitivity/recall) vs FP rate (1-specificity) and summarizes confusion matrices that each threshold produces\n",
    "- TP rate = recall\n",
    "- FP rate = FP/(FP+TN)\n",
    "- Diagonal (y = x) shows where TP rate = FP rate\n",
    "- optimal threshold are on left corners depending on how many FPs we’re willing to accept\n",
    "- AUC is used to compare ROC curves, larger AUC is better\n",
    "- Alternative to ROC: replace FP rate with Precision, not affected by class imbalance, so often more useful if we have more positives in the data\n",
    "\n",
    "\n",
    "### Odds/Log(Odds):\n",
    "- odds are not probabilities, but ratios of something happening to something not happening\n",
    "- probabilities are the ratios of something happening to everything happening\n",
    "- odds = P(A) / P’(A)\n",
    "    - 0 < odds < 1 if odds against A\n",
    "    - 1 < odds < inf if odds for A\n",
    "- Log(odds) creates symmetry:\n",
    "    - Negative if odds against A\n",
    "    - Positive if odds for A\n",
    "    - Can be calculated with probabilities: logit function = log(p/(1-p))\n",
    "    - Histogram of log odds is normally distributed which is useful for solving certain stats problems\n",
    "\n",
    "### Odds Ratio/ Log(Odds Ratio)\n",
    "- odds ratio = oddsA / oddsB\n",
    "    - 0 < odds < 1 if denominator> numerator\n",
    "    - 1 < odds < inf if numerator > denominator\n",
    "- Log(odds ratio) creates symmetry:\n",
    "    - Negative if oddsB > oddsA\n",
    "    - Positive if oddsA > oddsB\n",
    "- can see these ratios in confusion matrices with true positive rate and false positive rate\n",
    "- Like R-squared because these ratios indicate a relationship between two things (ex. Actual values vs predicted values) and these ratios correspond to effect size. A larger odds ratio (or log(odds ratio) means that predicted values are a good predictor of actual values and a smaller odds ratio means the predicted values are not a good predictor of the actual values\n",
    "- 3 ways to determine if odds ratio (or log(odds ratio) is statistically significant: \n",
    "    - Fischers Exact Test\n",
    "    - Chi-Squared Test (for p-value)\n",
    "    - The Wald Test (for confidence interval and p-value)\n",
    "        - Wald test see how many std’s the log(odds) is from zero\n",
    "        - std = sqrt(sum(1/TP+1/TN+1/FP+1/FN))\n",
    "\n",
    "### Logistic Regression\n",
    "- fits logistic function instead of a line\n",
    "- for function line, y is probability of class given X\n",
    "- class predicted by X with classification threshold 0 < p < 1\n",
    "- Unlike normal regression, we can’t easily compare the complex model (multiple features) to the simple model\n",
    "- Use Wald’s test to test if the variable’s effect on the prediction is significantly different than 0. If not, it means the variable is mot helping the prediction \n",
    "- MLE is used to pick the best logistic regression line\n",
    "\n",
    "### Likelihood vs Probability:\n",
    "- Probability: P(data | distribution), area under distribution curve\n",
    "- Likelihood: P(distribution | data) ex. P(mean =34g and std=2.5g | 34g object)\n",
    "\n",
    "### Log Reg Details Coeffs:\n",
    "- Generalized Linear Models (GLM): Log Reg and Linear Models\n",
    "- y-acis is transformed from a 0 to 1 range to a -inf to inf range with the logit function. y-axis goes from P(A) scale to a log(odds of A) scale\n",
    "- coefficients represent this new line:\n",
    "- Intercept: when x = 0, the log(odds of A) is ‘coeff’. If ‘coeff’ is negative, the odds are against A, if it’s positive, the odds are in favor of A\n",
    "- Wald’s Test: z-value is the estimated intercept/slope divided by the standard error (SE for the estimated intercept/slope), or the number of std’s the estimated intercept/slope is away from away from 0 on a standard normal curve. If |z-value| is less than 2, it is not statistically significant from 0, and should be confirmed by a larger p-value.\n",
    "- variable coeffs: for a one unit increase in x, the log(oddsA) increases by ‘coeff’\n",
    "- Log reg on categorical variables is similar to how we do a t-test usine linear models\n",
    "- “Intercept” the log(oddsA) for categorical vaiable not true/existing\n",
    "- “Cat coeff” term is the log(odds ratio) that tells you how much the category being true increases or decreases log(oddsA)\n",
    "- Coefficients are the same as in a linear model except that they are on a log(odds) scale\n",
    "\n",
    "### Log Reg Details MLE:\n",
    "- 1. Project each point onto candidate line (line that is transformed with logit function), giving each point a log(oddsA) value  (log(oddsA) is on y axis\n",
    "- 2. These log(oddsA) values are transformed to probability values using e^log(oddsA) / 1 + e^log(oddsA) (<- like inverse of logit function)\n",
    "- 3. Calculate the likelihood of A given the logistic regression line. Also remember the probability for A is also the y-value of this log reg line, not the area under the curve\n",
    "- 4. The likelihood for all A is the product of the individual A likelihoods\n",
    "- 5. The likelihood for A’ is 1 minus the y-value (on the log reg line) and, again, the likelihood for all A’ is the product of the individual A’ likelihoods\n",
    "- 6. The likelihood for all A and all A’ is the product of both. Statisticians like to calculate the log of the likelihood (log() each of the individual likelihoods), but either way, the same log reg lin3 maximizes both of these.\n",
    "\n",
    "### Log Reg Details R_squared and p-values:\n",
    "- McFadden’s pseudo R^2 = ( LL(overallprobability) - LogLikelihood(fittedline) ) / ( LL(overallprobabilty) - LL(saturated model) )\n",
    "- p-value = chi-squared value = 2 ( ( LL(saturated model) - LL(fit) ) - ( LL(saturated model) - LL(overall probability) ) )\n",
    "\n",
    "### (Multinomial) Naive Bayes\n",
    "- ed\n",
    "\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "- ed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
